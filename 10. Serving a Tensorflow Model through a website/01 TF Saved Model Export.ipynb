{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/faiqueali/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from time import strftime\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_PATH = 'MNIST/digit_xtrain.csv'\n",
    "X_TEST_PATH = 'MNIST/digit_xtest.csv'\n",
    "Y_TRAIN_PATH = 'MNIST/digit_ytrain.csv'\n",
    "Y_TEST_PATH = 'MNIST/digit_ytest.csv'\n",
    "\n",
    "LOGGING_PATH = 'tensorboard_mnist_digits_logs/'\n",
    "\n",
    "NR_CLASSES = 10\n",
    "VALIDATION_SIZE = 10000\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "CHANNELS = 1  # Grayscale\n",
    "TOTAL_INPUTS = IMAGE_WIDTH * IMAGE_HEIGHT * CHANNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 163 ms, sys: 3.12 ms, total: 166 ms\n",
      "Wall time: 166 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.loadtxt(Y_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.3 s, sys: 730 ms, total: 30 s\n",
      "Wall time: 29.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "x_train_all = np.loadtxt(X_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.83 s, sys: 92.1 ms, total: 4.92 s\n",
      "Wall time: 4.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "x_test = np.loadtxt(X_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each label corresponds to the categories or the classes for digits\n",
    "y_train_all[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-scale\n",
    "x_train_all, x_test = x_train_all / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert target values to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eg to convert sparse matrix to full matrix\n",
    "# Array element indexing in actual\n",
    "values = y_train_all[:5]\n",
    "np.eye(10)[values]\n",
    "\n",
    "# Before -> array([5, 0, 4, 1, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = np.eye(NR_CLASSES)[y_train_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.eye(NR_CLASSES)[y_test]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create validation dataset from training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge:** Split the training dataset into a smaller training dataset and a validation dataset for the features and the labels. Create four arrays: `x_val`, `y_val`, `x_train` and `y_train` from `x_train_all` and `y_train_all`. Use the validation size of 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train_all[:VALIDATION_SIZE]\n",
    "y_val = y_train_all[:VALIDATION_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_all[VALIDATION_SIZE:]\n",
    "y_train = y_train_all[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\n",
    "    tf.float32, \n",
    "    shape=[None, TOTAL_INPUTS],  # [How many samples would be going to use, 784->Total features]\n",
    "    name='X'\n",
    ")\n",
    "\n",
    "Y = tf.placeholder(tf.float32, shape=[None, NR_CLASSES], name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architecture\n",
    "\n",
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs = 50\n",
    "learning_rate = 1e-3  # 0.0001\n",
    "\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# For grouping info\\nwith tf.name_scope('hidden_1'):\\n\\n    # Initial weights of first hidden layer\\n    initial_w1 = tf.truncated_normal(\\n        shape=[TOTAL_INPUTS, n_hidden1], \\n        stddev=0.1,   # Far or close to each other\\n        seed=42)\\n\\n    # Creating the weights\\n    w1 = tf.Variable(initial_value = initial_w1, name='w1')\\n\\n    # Initialize the biases of the first hidden layer\\n    initial_b1 = tf.constant(value=0.0, shape=[n_hidden1])\\n    b1 = tf.Variable(initial_value = initial_b1, name='b1')\\n\\n    # Feature going into the first hidden layer (Multiplication Matrix)\\n    layer1_in = tf.matmul(X, w1) + b1\\n\\n    # Output of layer 1\\n    layer1_out = tf.nn.relu(layer1_in)\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# For grouping info\n",
    "with tf.name_scope('hidden_1'):\n",
    "\n",
    "    # Initial weights of first hidden layer\n",
    "    initial_w1 = tf.truncated_normal(\n",
    "        shape=[TOTAL_INPUTS, n_hidden1], \n",
    "        stddev=0.1,   # Far or close to each other\n",
    "        seed=42)\n",
    "\n",
    "    # Creating the weights\n",
    "    w1 = tf.Variable(initial_value = initial_w1, name='w1')\n",
    "\n",
    "    # Initialize the biases of the first hidden layer\n",
    "    initial_b1 = tf.constant(value=0.0, shape=[n_hidden1])\n",
    "    b1 = tf.Variable(initial_value = initial_b1, name='b1')\n",
    "\n",
    "    # Feature going into the first hidden layer (Multiplication Matrix)\n",
    "    layer1_in = tf.matmul(X, w1) + b1\n",
    "\n",
    "    # Output of layer 1\n",
    "    layer1_out = tf.nn.relu(layer1_in)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge:** Set up the second hidden layer. This layer has 64 neurons and needs to work off the output of the first hidden layer(see above). Then setup the output layer. Remember, the output layer will use the softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# For grouping info\\nwith tf.name_scope('hidden_2'):\\n\\n    # Initial weights of second hidden layer\\n    initial_w2 = tf.truncated_normal(shape=[n_hidden1, n_hidden2], stddev=0.1, seed=42)\\n    w2 = tf.Variable(initial_value = initial_w2, name='w2')\\n\\n    # Initialize the biases of the second hidden layer\\n    initial_b2 = tf.constant(value=0.0, shape=[n_hidden2])\\n    b2 = tf.Variable(initial_value = initial_b2, name='b2')\\n\\n    # Feature going into the second hidden layer (Multiplication Matrix)\\n    layer2_in = tf.matmul(layer1_out, w2) + b2\\n\\n    # Output of layer 2\\n    layer2_out = tf.nn.relu(layer2_in)\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# For grouping info\n",
    "with tf.name_scope('hidden_2'):\n",
    "\n",
    "    # Initial weights of second hidden layer\n",
    "    initial_w2 = tf.truncated_normal(shape=[n_hidden1, n_hidden2], stddev=0.1, seed=42)\n",
    "    w2 = tf.Variable(initial_value = initial_w2, name='w2')\n",
    "\n",
    "    # Initialize the biases of the second hidden layer\n",
    "    initial_b2 = tf.constant(value=0.0, shape=[n_hidden2])\n",
    "    b2 = tf.Variable(initial_value = initial_b2, name='b2')\n",
    "\n",
    "    # Feature going into the second hidden layer (Multiplication Matrix)\n",
    "    layer2_in = tf.matmul(layer1_out, w2) + b2\n",
    "\n",
    "    # Output of layer 2\n",
    "    layer2_out = tf.nn.relu(layer2_in)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# For grouping info\\nwith tf.name_scope('output_layer'):\\n\\n    # Initial weights of output layer\\n    initial_w3 = tf.truncated_normal(shape=[n_hidden2, NR_CLASSES], stddev=0.1, seed=42)\\n    w3 = tf.Variable(initial_value = initial_w3, name='w3')\\n\\n    # Initialize the biases of the output layer\\n    initial_b3 = tf.constant(value=0.0, shape=[NR_CLASSES])\\n    b3 = tf.Variable(initial_value = initial_b3, name='b3')\\n\\n    # Feature going into the output layer (Multiplication Matrix)\\n    layer3_in = tf.matmul(layer2_out, w3) + b3\\n\\n    # Output layer\\n    output = tf.nn.softmax(layer3_in)\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# For grouping info\n",
    "with tf.name_scope('output_layer'):\n",
    "\n",
    "    # Initial weights of output layer\n",
    "    initial_w3 = tf.truncated_normal(shape=[n_hidden2, NR_CLASSES], stddev=0.1, seed=42)\n",
    "    w3 = tf.Variable(initial_value = initial_w3, name='w3')\n",
    "\n",
    "    # Initialize the biases of the output layer\n",
    "    initial_b3 = tf.constant(value=0.0, shape=[NR_CLASSES])\n",
    "    b3 = tf.Variable(initial_value = initial_b3, name='b3')\n",
    "\n",
    "    # Feature going into the output layer (Multiplication Matrix)\n",
    "    layer3_in = tf.matmul(layer2_out, w3) + b3\n",
    "\n",
    "    # Output layer\n",
    "    output = tf.nn.softmax(layer3_in)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_layer(input, weight_dim, bias_dim, name):\n",
    "    \n",
    "    # For grouping info\n",
    "    with tf.name_scope(name):\n",
    "        \n",
    "        # Initialize weights of output layer\n",
    "        initial_w = tf.truncated_normal(shape=weight_dim, stddev=0.1, seed=42)\n",
    "        w = tf.Variable(initial_value = initial_w, name='W')\n",
    "\n",
    "        # Initialize the biases of the output layer\n",
    "        initial_b = tf.constant(value=0.0, shape=bias_dim)\n",
    "        b = tf.Variable(initial_value = initial_b, name='B')\n",
    "\n",
    "        # Feature going into the output layer (Multiplication Matrix)\n",
    "        layer_in = tf.matmul(input, w) + b\n",
    "\n",
    "        # Output layer\n",
    "        if name == 'out':\n",
    "            layer_out = tf.nn.softmax(layer_in)\n",
    "        else:\n",
    "            layer_out = tf.nn.relu(layer_in)\n",
    "            \n",
    "        # For histogram summart\n",
    "        tf.summary.histogram('weights',w)\n",
    "        tf.summary.histogram('biases',b)\n",
    "            \n",
    "        return layer_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# MODEL WITHOUT DROPOUT\\nlayer_1 = setup_layer(X, weight_dim = [TOTAL_INPUTS, n_hidden1], \\n                      bias_dim=[n_hidden1], name='layer_1')\\n\\nlayer_2 = setup_layer(layer_1, weight_dim = [n_hidden1, n_hidden2], \\n                      bias_dim=[n_hidden2], name='layer_2')\\n\\noutput = setup_layer(layer_2, weight_dim = [n_hidden2, NR_CLASSES], \\n                      bias_dim=[NR_CLASSES], name='out')\\n\\nmodel_name = f'{n_hidden1}-{n_hidden2} LR{learning_rate} E{nr_epochs}'\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# MODEL WITHOUT DROPOUT\n",
    "layer_1 = setup_layer(X, weight_dim = [TOTAL_INPUTS, n_hidden1], \n",
    "                      bias_dim=[n_hidden1], name='layer_1')\n",
    "\n",
    "layer_2 = setup_layer(layer_1, weight_dim = [n_hidden1, n_hidden2], \n",
    "                      bias_dim=[n_hidden2], name='layer_2')\n",
    "\n",
    "output = setup_layer(layer_2, weight_dim = [n_hidden2, NR_CLASSES], \n",
    "                      bias_dim=[NR_CLASSES], name='out')\n",
    "\n",
    "model_name = f'{n_hidden1}-{n_hidden2} LR{learning_rate} E{nr_epochs}'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/faiqueali/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "layer_1 = setup_layer(X, weight_dim = [TOTAL_INPUTS, n_hidden1], \n",
    "                      bias_dim=[n_hidden1], name='layer_1')\n",
    "\n",
    "layer_drop = tf.nn.dropout(layer_1, keep_prob=0.8, name='dropout_layer')\n",
    "\n",
    "layer_2 = setup_layer(layer_drop, weight_dim = [n_hidden1, n_hidden2], \n",
    "                      bias_dim=[n_hidden2], name='layer_2')\n",
    "\n",
    "output = setup_layer(layer_2, weight_dim = [n_hidden2, NR_CLASSES], \n",
    "                      bias_dim=[NR_CLASSES], name='out')\n",
    "\n",
    "model_name = f'{n_hidden1}-{n_hidden2} LR{learning_rate} E{nr_epochs}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created directories !!\n"
     ]
    }
   ],
   "source": [
    "# Folder for tensorboard\n",
    "\n",
    "folder_name = f'{model_name} at {strftime(\"%H:%M\")}'\n",
    "directory = os.path.join(LOGGING_PATH, folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as exception:\n",
    "    print(exception.strerror)\n",
    "else:\n",
    "    print('Successfully created directories !!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, Optimisation & Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Loss Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss_cal'):\n",
    "    # Computes softmax cross entropy between logits and labels\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    # Initialize the optimizer and specify 'adam' to use and define learning rate\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    # Specify operation what the optimizer will do to minimize the loss\n",
    "    train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy_calc'):\n",
    "    model_prediction = tf.argmax(output, axis=1, name='prediction')\n",
    "    \n",
    "    #argmax with extract the index of highest value(1) from each row\n",
    "    correct_pred = tf.equal(model_prediction, tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('performance'):\n",
    "    # Creating Tensorboard Summaries\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cost', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Input Images in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('show_image'):\n",
    "    # All data in placeholder\n",
    "    x_image = tf.reshape(X, [-1,28,28,1])\n",
    "    tf.summary.image('image_input', x_image, max_outputs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Filewriter and Merge Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter(directory + '/train')\n",
    "train_writer.add_graph(sess.graph)\n",
    "\n",
    "validation_writer = tf.summary.FileWriter(directory + '/validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old part code\n",
    "# b3.eval(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = y_train.shape[0]\n",
    "nr_iterations = int(num_examples / size_of_batch)\n",
    "\n",
    "# To track where a batch ends and other starts\n",
    "index_in_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    \n",
    "    # To acess global variables to get their current values\n",
    "    global num_examples\n",
    "    global index_in_epoch\n",
    "    \n",
    "    # Starting value and incrementing index\n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # To reset\n",
    "    if index_in_epoch > num_examples:\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "    \n",
    "    end = index_in_epoch\n",
    "    \n",
    "    # Returing all values(data) & features(labels) between starting and ending batch value\n",
    "    return data[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t| Training Accuracy = 0.847000002861023\n",
      "Epoch 1 \t| Training Accuracy = 0.859000027179718\n",
      "Epoch 2 \t| Training Accuracy = 0.8659999966621399\n",
      "Epoch 3 \t| Training Accuracy = 0.8740000128746033\n",
      "Epoch 4 \t| Training Accuracy = 0.875\n",
      "Epoch 5 \t| Training Accuracy = 0.8740000128746033\n",
      "Epoch 6 \t| Training Accuracy = 0.9760000109672546\n",
      "Epoch 7 \t| Training Accuracy = 0.9810000061988831\n",
      "Epoch 8 \t| Training Accuracy = 0.9810000061988831\n",
      "Epoch 9 \t| Training Accuracy = 0.9819999933242798\n",
      "Epoch 10 \t| Training Accuracy = 0.9819999933242798\n",
      "Epoch 11 \t| Training Accuracy = 0.9850000143051147\n",
      "Epoch 12 \t| Training Accuracy = 0.9869999885559082\n",
      "Epoch 13 \t| Training Accuracy = 0.9869999885559082\n",
      "Epoch 14 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 15 \t| Training Accuracy = 0.9879999756813049\n",
      "Epoch 16 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 17 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 18 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 19 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 20 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 21 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 22 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 23 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 24 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 25 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 26 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 27 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 28 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 29 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 30 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 31 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 32 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 33 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 34 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 35 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 36 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 37 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 38 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 39 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 40 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 41 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 42 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 43 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 44 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 45 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 46 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 47 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 48 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 49 \t| Training Accuracy = 0.9929999709129333\n",
      "Done training !!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nr_epochs):\n",
    "    \n",
    "    # ===================== TRAINING DATASET =====================\n",
    "    for i in range(nr_iterations):\n",
    "        # 1- getting batch data and features\n",
    "        batch_x, batch_y = next_batch(batch_size= size_of_batch, data=x_train, labels=y_train)\n",
    "        \n",
    "        # 2- To feed it to our session\n",
    "        feed_dictionary = {X:batch_x, Y:batch_y}\n",
    "        \n",
    "        # To do calculations\n",
    "        sess.run(train_step, feed_dict=feed_dictionary)\n",
    "        \n",
    "    # Fetching merged summary and accuracy from session\n",
    "    s, batch_accuracy = sess.run(fetches=[merged_summary, accuracy], feed_dict=feed_dictionary)\n",
    "    \n",
    "    # Writing summary to file\n",
    "    train_writer.add_summary(s, epoch)\n",
    "        \n",
    "    print(f'Epoch {epoch} \\t| Training Accuracy = {batch_accuracy}')\n",
    "    \n",
    "    \n",
    "    # ===================== VALIDATION =========================\n",
    "    summary = sess.run(fetches=merged_summary, feed_dict={X: x_val, Y: y_val})\n",
    "    validation_writer.add_summary(summary, epoch)\n",
    "    \n",
    "    \n",
    "print('Done training !!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-45-fc1d61da332f>:3: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
      "WARNING:tensorflow:From /Users/faiqueali/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:200: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: SavedModel/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "outputs = {'accuracy_calc/prediction': model_prediction}\n",
    "inputs = {'X': X}\n",
    "tf.compat.v1.saved_model.simple_save(sess, 'SavedModel', inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAYAAAByDd+UAAABAElEQVR4nO2Vyw2DMAyG3ap3RoAVmMCBrXICbwIbMEEEmzBCmMA9VFR9EGLTqhIV3xET/vjHjxMzM/yQ8y/FDsEgRAREtEnwJC2aWWAYhqfnzjmVoChDIroLISI45+5C2kwvsRemaYK+76HrOkiS5CmGiG8ZR2EB4zgGY0VRcF3Xks8wM7PI0jRNgzFEVCW4j7b4CLH5C3jv2RjD3nvxmWiVhphbJc/zt+pdY5NgWZYAcCuYqqp0hzUWWmvZGMPWWrX9KksfJ03TNKttEmNV8HWkqe1bIDi8iQjatoUsy8TNLbnQ6rbQDObZiej22Pz3F5DMVPE+/Bb/P0sPwf0LXgGAJwNqzP5nHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=28x28 at 0x7FF954BA8AC0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('MNIST/test_img.png')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting image to black and white\n",
    "bw = img.convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = np.invert(bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert 2D(img_array) to flatten\n",
    "test_img = img_array.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = sess.run(fetches= tf.argmax(output, axis=1),\n",
    "                      feed_dict={X:[test_img]}) #will not supply y, dont care about accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test image is [2]\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction for test image is {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge:** Calculate the accuracy over the test dataset(`x_test` and `y_test`). Use your knowledge of running a session to get the accuracy. Display the accuracy as a percentage rounded to two decimal numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is 97.58%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = sess.run(fetches=accuracy, feed_dict={X:x_test, Y:y_test})\n",
    "print(f'Accuracy on test set is {test_accuracy:0.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset for the Next Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer.close()\n",
    "validation_writer.close()\n",
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
