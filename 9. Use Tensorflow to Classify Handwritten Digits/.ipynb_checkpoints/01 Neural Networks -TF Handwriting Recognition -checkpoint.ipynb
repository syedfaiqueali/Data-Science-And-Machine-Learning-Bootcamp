{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_PATH = 'MNIST/digit_xtrain.csv'\n",
    "X_TEST_PATH = 'MNIST/digit_xtest.csv'\n",
    "Y_TRAIN_PATH = 'MNIST/digit_ytrain.csv'\n",
    "Y_TEST_PATH = 'MNIST/digit_ytest.csv'\n",
    "\n",
    "LOGGING_PATH = 'tensorboard_mnist_digits_logs/'\n",
    "\n",
    "NR_CLASSES = 10\n",
    "VALIDATION_SIZE = 10000\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "CHANNELS = 1  # Grayscale\n",
    "TOTAL_INPUTS = IMAGE_WIDTH * IMAGE_HEIGHT * CHANNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 168 ms, sys: 3.43 ms, total: 171 ms\n",
      "Wall time: 170 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.loadtxt(Y_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 s, sys: 816 ms, total: 31.9 s\n",
      "Wall time: 31.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "x_train_all = np.loadtxt(X_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.11 s, sys: 125 ms, total: 5.24 s\n",
      "Wall time: 5.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "x_test = np.loadtxt(X_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each label corresponds to the categories or the classes for digits\n",
    "y_train_all[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-scale\n",
    "x_train_all, x_test = x_train_all / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert target values to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eg to convert sparse matrix to full matrix\n",
    "# Array element indexing in actual\n",
    "values = y_train_all[:5]\n",
    "np.eye(10)[values]\n",
    "\n",
    "# Before -> array([5, 0, 4, 1, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = np.eye(NR_CLASSES)[y_train_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.eye(NR_CLASSES)[y_test]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create validation dataset from training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge:** Split the training dataset into a smaller training dataset and a validation dataset for the features and the labels. Create four arrays: `x_val`, `y_val`, `x_train` and `y_train` from `x_train_all` and `y_train_all`. Use the validation size of 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train_all[:VALIDATION_SIZE]\n",
    "y_val = y_train_all[:VALIDATION_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_all[VALIDATION_SIZE:]\n",
    "y_train = y_train_all[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\n",
    "    tf.float32, \n",
    "    shape=[None, TOTAL_INPUTS],  # [How many samples would be going to use, 784->Total features]\n",
    "    name='X'\n",
    ")\n",
    "\n",
    "Y = tf.placeholder(tf.float32, shape=[None, NR_CLASSES], name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architecture\n",
    "\n",
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs = 5\n",
    "learning_rate = 1e-4  # 0.0001\n",
    "\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# For grouping info\\nwith tf.name_scope('hidden_1'):\\n\\n    # Initial weights of first hidden layer\\n    initial_w1 = tf.truncated_normal(\\n        shape=[TOTAL_INPUTS, n_hidden1], \\n        stddev=0.1,   # Far or close to each other\\n        seed=42)\\n\\n    # Creating the weights\\n    w1 = tf.Variable(initial_value = initial_w1, name='w1')\\n\\n    # Initialize the biases of the first hidden layer\\n    initial_b1 = tf.constant(value=0.0, shape=[n_hidden1])\\n    b1 = tf.Variable(initial_value = initial_b1, name='b1')\\n\\n    # Feature going into the first hidden layer (Multiplication Matrix)\\n    layer1_in = tf.matmul(X, w1) + b1\\n\\n    # Output of layer 1\\n    layer1_out = tf.nn.relu(layer1_in)\\n\""
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# For grouping info\n",
    "with tf.name_scope('hidden_1'):\n",
    "\n",
    "    # Initial weights of first hidden layer\n",
    "    initial_w1 = tf.truncated_normal(\n",
    "        shape=[TOTAL_INPUTS, n_hidden1], \n",
    "        stddev=0.1,   # Far or close to each other\n",
    "        seed=42)\n",
    "\n",
    "    # Creating the weights\n",
    "    w1 = tf.Variable(initial_value = initial_w1, name='w1')\n",
    "\n",
    "    # Initialize the biases of the first hidden layer\n",
    "    initial_b1 = tf.constant(value=0.0, shape=[n_hidden1])\n",
    "    b1 = tf.Variable(initial_value = initial_b1, name='b1')\n",
    "\n",
    "    # Feature going into the first hidden layer (Multiplication Matrix)\n",
    "    layer1_in = tf.matmul(X, w1) + b1\n",
    "\n",
    "    # Output of layer 1\n",
    "    layer1_out = tf.nn.relu(layer1_in)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge:** Set up the second hidden layer. This layer has 64 neurons and needs to work off the output of the first hidden layer(see above). Then setup the output layer. Remember, the output layer will use the softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# For grouping info\\nwith tf.name_scope('hidden_2'):\\n\\n    # Initial weights of second hidden layer\\n    initial_w2 = tf.truncated_normal(shape=[n_hidden1, n_hidden2], stddev=0.1, seed=42)\\n    w2 = tf.Variable(initial_value = initial_w2, name='w2')\\n\\n    # Initialize the biases of the second hidden layer\\n    initial_b2 = tf.constant(value=0.0, shape=[n_hidden2])\\n    b2 = tf.Variable(initial_value = initial_b2, name='b2')\\n\\n    # Feature going into the second hidden layer (Multiplication Matrix)\\n    layer2_in = tf.matmul(layer1_out, w2) + b2\\n\\n    # Output of layer 2\\n    layer2_out = tf.nn.relu(layer2_in)\\n\""
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# For grouping info\n",
    "with tf.name_scope('hidden_2'):\n",
    "\n",
    "    # Initial weights of second hidden layer\n",
    "    initial_w2 = tf.truncated_normal(shape=[n_hidden1, n_hidden2], stddev=0.1, seed=42)\n",
    "    w2 = tf.Variable(initial_value = initial_w2, name='w2')\n",
    "\n",
    "    # Initialize the biases of the second hidden layer\n",
    "    initial_b2 = tf.constant(value=0.0, shape=[n_hidden2])\n",
    "    b2 = tf.Variable(initial_value = initial_b2, name='b2')\n",
    "\n",
    "    # Feature going into the second hidden layer (Multiplication Matrix)\n",
    "    layer2_in = tf.matmul(layer1_out, w2) + b2\n",
    "\n",
    "    # Output of layer 2\n",
    "    layer2_out = tf.nn.relu(layer2_in)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# For grouping info\\nwith tf.name_scope('output_layer'):\\n\\n    # Initial weights of output layer\\n    initial_w3 = tf.truncated_normal(shape=[n_hidden2, NR_CLASSES], stddev=0.1, seed=42)\\n    w3 = tf.Variable(initial_value = initial_w3, name='w3')\\n\\n    # Initialize the biases of the output layer\\n    initial_b3 = tf.constant(value=0.0, shape=[NR_CLASSES])\\n    b3 = tf.Variable(initial_value = initial_b3, name='b3')\\n\\n    # Feature going into the output layer (Multiplication Matrix)\\n    layer3_in = tf.matmul(layer2_out, w3) + b3\\n\\n    # Output layer\\n    output = tf.nn.softmax(layer3_in)\\n\""
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# For grouping info\n",
    "with tf.name_scope('output_layer'):\n",
    "\n",
    "    # Initial weights of output layer\n",
    "    initial_w3 = tf.truncated_normal(shape=[n_hidden2, NR_CLASSES], stddev=0.1, seed=42)\n",
    "    w3 = tf.Variable(initial_value = initial_w3, name='w3')\n",
    "\n",
    "    # Initialize the biases of the output layer\n",
    "    initial_b3 = tf.constant(value=0.0, shape=[NR_CLASSES])\n",
    "    b3 = tf.Variable(initial_value = initial_b3, name='b3')\n",
    "\n",
    "    # Feature going into the output layer (Multiplication Matrix)\n",
    "    layer3_in = tf.matmul(layer2_out, w3) + b3\n",
    "\n",
    "    # Output layer\n",
    "    output = tf.nn.softmax(layer3_in)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_layer(input, weight_dim, bias_dim, name):\n",
    "    \n",
    "    # For grouping info\n",
    "    with tf.name_scope(name):\n",
    "        \n",
    "        # Initial weights of output layer\n",
    "        initial_w = tf.truncated_normal(shape=weight_dim, stddev=0.1, seed=42)\n",
    "        w = tf.Variable(initial_value = initial_w, name='W')\n",
    "\n",
    "        # Initialize the biases of the output layer\n",
    "        initial_b = tf.constant(value=0.0, shape=bias_dim)\n",
    "        b = tf.Variable(initial_value = initial_b, name='B')\n",
    "\n",
    "        # Feature going into the output layer (Multiplication Matrix)\n",
    "        layer_in = tf.matmul(input, w) + b\n",
    "\n",
    "        # Output layer\n",
    "        if name == 'out':\n",
    "            layer_out = tf.nn.softmax(layer_in)\n",
    "        else:\n",
    "            layer_out = tf.nn.relu(layer_in)\n",
    "            \n",
    "        return layer_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = setup_layer(X, weight_dim = [TOTAL_INPUTS, n_hidden1], \n",
    "                      bias_dim=[n_hidden1], name='layer_1')\n",
    "\n",
    "layer_2 = setup_layer(layer_1, weight_dim = [n_hidden1, n_hidden2], \n",
    "                      bias_dim=[n_hidden2], name='layer_2')\n",
    "\n",
    "output = setup_layer(layer_2, weight_dim = [n_hidden2, NR_CLASSES], \n",
    "                      bias_dim=[NR_CLASSES], name='out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created directories !!\n"
     ]
    }
   ],
   "source": [
    "# Folder for tensorboard\n",
    "\n",
    "folder_name = f'Model 1 at {strftime(\"%H:%M\")}'\n",
    "directory = os.path.join(LOGGING_PATH, folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as exception:\n",
    "    print(exception.strerror)\n",
    "else:\n",
    "    print('Successfully created directories !!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, Optimisation & Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Loss Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss_cal'):\n",
    "    # Computes softmax cross entropy between logits and labels\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    # Initialize the optimizer and specify 'adam' to use and define learning rate\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    # Specify operation what the optimizer will do to minimize the loss\n",
    "    train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy_calc'):\n",
    "    #argmax with extract the index of highest value(1) from each row\n",
    "    correct_pred = tf.equal(tf.argmax(output, axis=1), tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('performance'):\n",
    "    # Creating Tensorboard Summaries\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cost', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Input Images in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('show_image'):\n",
    "    # All data in placeholder\n",
    "    x_image = tf.reshape(X, [-1,28,28,1])\n",
    "    tf.summary.image('image_input', x_image, max_outputs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Filewriter and Merge Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter(directory + '/train')\n",
    "train_writer.add_graph(sess.graph)\n",
    "\n",
    "validation_writer = tf.summary.FileWriter(directory + '/validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old part code\n",
    "# b3.eval(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = y_train.shape[0]\n",
    "nr_iterations = int(num_examples / size_of_batch)\n",
    "\n",
    "# To track where a batch ends and other starts\n",
    "index_in_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    \n",
    "    # To acess global variables to get their current values\n",
    "    global num_examples\n",
    "    global index_in_epoch\n",
    "    \n",
    "    # Starting value and incrementing index\n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # To reset\n",
    "    if index_in_epoch > num_examples:\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "    \n",
    "    end = index_in_epoch\n",
    "    \n",
    "    # Returing all values(data) & features(labels) between starting and ending batch value\n",
    "    return data[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t| Training Accuracy = 0.4050000011920929\n",
      "Epoch 1 \t| Training Accuracy = 0.7329999804496765\n",
      "Epoch 2 \t| Training Accuracy = 0.8190000057220459\n",
      "Epoch 3 \t| Training Accuracy = 0.843999981880188\n",
      "Epoch 4 \t| Training Accuracy = 0.8560000061988831\n",
      "Done training !!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nr_epochs):\n",
    "    \n",
    "    # ===================== TRAINING DATASET =====================\n",
    "    for i in range(nr_iterations):\n",
    "        # 1- getting batch data and features\n",
    "        batch_x, batch_y = next_batch(batch_size= size_of_batch, data=x_train, labels=y_train)\n",
    "        \n",
    "        # 2- To feed it to our session\n",
    "        feed_dictionary = {X:batch_x, Y:batch_y}\n",
    "        \n",
    "        # To do calculations\n",
    "        sess.run(train_step, feed_dict=feed_dictionary)\n",
    "        \n",
    "    # Fetching merged summary and accuracy from session\n",
    "    s, batch_accuracy = sess.run(fetches=[merged_summary, accuracy], feed_dict=feed_dictionary)\n",
    "    \n",
    "    # Writing summary to file\n",
    "    train_writer.add_summary(s, epoch)\n",
    "        \n",
    "    print(f'Epoch {epoch} \\t| Training Accuracy = {batch_accuracy}')\n",
    "    \n",
    "    \n",
    "    # ===================== VALIDATION =========================\n",
    "    summary = sess.run(fetches=merged_summary, feed_dict={X: x_val, Y: y_val})\n",
    "    validation_writer.add_summary(summary, epoch)\n",
    "    \n",
    "    \n",
    "print('Done training !!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset for the Next Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer.close()\n",
    "validation_writer.close()\n",
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
