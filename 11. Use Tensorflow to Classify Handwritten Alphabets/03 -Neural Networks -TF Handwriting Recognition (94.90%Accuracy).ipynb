{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/faiqueali/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from time import strftime\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_PATH = 'NIST/alphabets_xtrain.csv'\n",
    "X_TEST_PATH = 'NIST/alphabets_xtest.csv'\n",
    "Y_TRAIN_PATH = 'NIST/alphabets_ytrain.csv'\n",
    "Y_TEST_PATH = 'NIST/alphabets_ytest.csv'\n",
    "\n",
    "LOGGING_PATH = 'tensorboard_mnist_digits_logs/'\n",
    "\n",
    "NR_CLASSES = 26\n",
    "VALIDATION_SIZE = 111735 #Same as test data set\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "CHANNELS = 1  # Grayscale\n",
    "TOTAL_INPUTS = IMAGE_WIDTH * IMAGE_HEIGHT * CHANNELS\n",
    "\n",
    "ALPHABETS = {\n",
    "    0:'A', 1:'B', 2:'C', 3:'D', 4:'E', 5:'F', 6:'G', 7:'H', 8:'I',9:'J',\n",
    "    10:'K', 11:'L', 12:'M', 13:'N', 14:'O', 15:'P', 16:'Q', 17:'R', 18:'S',\n",
    "    19:'T', 20:'U', 21:'V', 22:'W', 23:'X', 24: 'Y', 25:'Z'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 678 ms, sys: 5.18 ms, total: 683 ms\n",
      "Wall time: 685 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260715,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 293 ms, sys: 3.07 ms, total: 297 ms\n",
      "Wall time: 297 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_test = np.loadtxt(Y_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 25s, sys: 3.39 s, total: 2min 28s\n",
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_train_all = np.loadtxt(X_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 1.39 s, total: 1min 2s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_test = np.loadtxt(X_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260715, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  15,  29,  23,\n",
       "        71, 118, 114,  60,  79, 120, 135, 198, 121,  72,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  64, 101, 164,\n",
       "       189, 180, 212, 228, 227, 209, 215, 229, 234, 255, 199, 167,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  91, 204,\n",
       "       220, 243, 251, 248, 255, 237, 227, 244, 255, 255, 255, 255, 231,\n",
       "       168,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   5,\n",
       "       181, 254, 255, 255, 255, 196, 167,  92,  79, 127, 126, 154, 136,\n",
       "       171, 105,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   3, 127, 255, 255, 255, 255,  99,  46,   9,   4,  27,  23,\n",
       "        38,  28,  48,  14,  14,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,  44, 176, 241, 255, 255, 130,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   9,  54, 111, 190, 255, 218,  84,\n",
       "        27,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   7,  73, 203,\n",
       "       237, 205, 109,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         6,  80, 163, 249, 193,  37,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,  54, 182, 238, 130,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  71,  90,  63,\n",
       "        44,   4,   0,   0,   0,   0,   1, 125, 255, 215,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 192,\n",
       "       234, 191, 163,  80,  27,  61,  25,  52,  70, 191, 255, 207,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0, 217, 255, 255, 255, 217, 134, 195, 130, 179, 207, 255, 245,\n",
       "       122,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0, 166, 221, 249, 255, 255, 255, 255, 255, 255, 255,\n",
       "       241, 170,  44,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  44, 104, 183, 214, 228, 231, 246, 239,\n",
       "       228, 212, 157,  27,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  24,  75, 118, 128,\n",
       "       172, 149, 118,  71,   7,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260715,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 18, 12, 17,  6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260715, 784)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111735, 784)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111735,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-scale our features\n",
    "x_train_all, x_test = x_train_all/255.0, x_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Target values to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Sparse Matrix to full Matrix\n",
    "values = y_train_all[:5]\n",
    "np.eye(NR_CLASSES)[values]  #0-25(26 types of labels i.e Alphabets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pulling out 4th value of our training values\n",
    "values[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = np.eye(NR_CLASSES)[y_train_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260715, 26)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111735, 26)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.eye(NR_CLASSES)[y_test]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create validation dataset from training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge:** Split the training dataset into a smaller training dataset and a validation dataset for the features and the labels. Create four arrays: `x_val`, `y_val`, `x_train` and `y_train` from `x_train_all` and `y_train_all`. Use the validation size of 111,735."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train_all[:VALIDATION_SIZE]\n",
    "y_val = y_train_all[:VALIDATION_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_all[VALIDATION_SIZE:]\n",
    "y_train = y_train_all[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111735, 784)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148980, 784)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\n",
    "    tf.float32, \n",
    "    shape=[None, TOTAL_INPUTS],  # [How many samples would be going to use, 784->Total features]\n",
    "    name='X'\n",
    ")\n",
    "\n",
    "Y = tf.placeholder(tf.float32, shape=[None, NR_CLASSES], name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs = 50\n",
    "learning_rate = 1e-3  # 0.0001\n",
    "\n",
    "n_hidden1 = 1500\n",
    "n_hidden2 = 512\n",
    "n_hidden3 = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for 1st Part of Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# For grouping info\\nwith tf.name_scope('hidden_1'):\\n\\n    # Initial weights of first hidden layer\\n    initial_w1 = tf.truncated_normal(\\n        shape=[TOTAL_INPUTS, n_hidden1], \\n        stddev=0.1,   # Far or close to each other\\n        seed=42)\\n\\n    # Creating the weights\\n    w1 = tf.Variable(initial_value = initial_w1, name='w1')\\n\\n    # Initialize the biases of the first hidden layer\\n    initial_b1 = tf.constant(value=0.0, shape=[n_hidden1])\\n    b1 = tf.Variable(initial_value = initial_b1, name='b1')\\n\\n    # Feature going into the first hidden layer (Multiplication Matrix)\\n    layer1_in = tf.matmul(X, w1) + b1\\n\\n    # Output of layer 1\\n    layer1_out = tf.nn.relu(layer1_in)\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# For grouping info\n",
    "with tf.name_scope('hidden_1'):\n",
    "\n",
    "    # Initial weights of first hidden layer\n",
    "    initial_w1 = tf.truncated_normal(\n",
    "        shape=[TOTAL_INPUTS, n_hidden1], \n",
    "        stddev=0.1,   # Far or close to each other\n",
    "        seed=42)\n",
    "\n",
    "    # Creating the weights\n",
    "    w1 = tf.Variable(initial_value = initial_w1, name='w1')\n",
    "\n",
    "    # Initialize the biases of the first hidden layer\n",
    "    initial_b1 = tf.constant(value=0.0, shape=[n_hidden1])\n",
    "    b1 = tf.Variable(initial_value = initial_b1, name='b1')\n",
    "\n",
    "    # Feature going into the first hidden layer (Multiplication Matrix)\n",
    "    layer1_in = tf.matmul(X, w1) + b1\n",
    "\n",
    "    # Output of layer 1\n",
    "    layer1_out = tf.nn.relu(layer1_in)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# For grouping info\\nwith tf.name_scope('hidden_2'):\\n\\n    # Initial weights of second hidden layer\\n    initial_w2 = tf.truncated_normal(shape=[n_hidden1, n_hidden2], stddev=0.1, seed=42)\\n    w2 = tf.Variable(initial_value = initial_w2, name='w2')\\n\\n    # Initialize the biases of the second hidden layer\\n    initial_b2 = tf.constant(value=0.0, shape=[n_hidden2])\\n    b2 = tf.Variable(initial_value = initial_b2, name='b2')\\n\\n    # Feature going into the second hidden layer (Multiplication Matrix)\\n    layer2_in = tf.matmul(layer1_out, w2) + b2\\n\\n    # Output of layer 2\\n    layer2_out = tf.nn.relu(layer2_in)\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# For grouping info\n",
    "with tf.name_scope('hidden_2'):\n",
    "\n",
    "    # Initial weights of second hidden layer\n",
    "    initial_w2 = tf.truncated_normal(shape=[n_hidden1, n_hidden2], stddev=0.1, seed=42)\n",
    "    w2 = tf.Variable(initial_value = initial_w2, name='w2')\n",
    "\n",
    "    # Initialize the biases of the second hidden layer\n",
    "    initial_b2 = tf.constant(value=0.0, shape=[n_hidden2])\n",
    "    b2 = tf.Variable(initial_value = initial_b2, name='b2')\n",
    "\n",
    "    # Feature going into the second hidden layer (Multiplication Matrix)\n",
    "    layer2_in = tf.matmul(layer1_out, w2) + b2\n",
    "\n",
    "    # Output of layer 2\n",
    "    layer2_out = tf.nn.relu(layer2_in)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# For grouping info\\nwith tf.name_scope('output_layer'):\\n\\n    # Initial weights of output layer\\n    initial_w3 = tf.truncated_normal(shape=[n_hidden2, NR_CLASSES], stddev=0.1, seed=42)\\n    w3 = tf.Variable(initial_value = initial_w3, name='w3')\\n\\n    # Initialize the biases of the output layer\\n    initial_b3 = tf.constant(value=0.0, shape=[NR_CLASSES])\\n    b3 = tf.Variable(initial_value = initial_b3, name='b3')\\n\\n    # Feature going into the output layer (Multiplication Matrix)\\n    layer3_in = tf.matmul(layer2_out, w3) + b3\\n\\n    # Output layer\\n    output = tf.nn.softmax(layer3_in)\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# For grouping info\n",
    "with tf.name_scope('output_layer'):\n",
    "\n",
    "    # Initial weights of output layer\n",
    "    initial_w3 = tf.truncated_normal(shape=[n_hidden2, NR_CLASSES], stddev=0.1, seed=42)\n",
    "    w3 = tf.Variable(initial_value = initial_w3, name='w3')\n",
    "\n",
    "    # Initialize the biases of the output layer\n",
    "    initial_b3 = tf.constant(value=0.0, shape=[NR_CLASSES])\n",
    "    b3 = tf.Variable(initial_value = initial_b3, name='b3')\n",
    "\n",
    "    # Feature going into the output layer (Multiplication Matrix)\n",
    "    layer3_in = tf.matmul(layer2_out, w3) + b3\n",
    "\n",
    "    # Output layer\n",
    "    output = tf.nn.softmax(layer3_in)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_layer(input, weight_dim, bias_dim, name):\n",
    "    \n",
    "    # For grouping info\n",
    "    with tf.name_scope(name):\n",
    "        \n",
    "        # Initialize weights of output layer\n",
    "        initial_w = tf.truncated_normal(shape=weight_dim, stddev=0.1, seed=42)\n",
    "        w = tf.Variable(initial_value = initial_w, name='W')\n",
    "\n",
    "        # Initialize the biases of the output layer\n",
    "        initial_b = tf.constant(value=0.0, shape=bias_dim)\n",
    "        b = tf.Variable(initial_value = initial_b, name='B')\n",
    "\n",
    "        # Feature going into the output layer (Multiplication Matrix)\n",
    "        layer_in = tf.matmul(input, w) + b\n",
    "\n",
    "        # Output layer\n",
    "        if name == 'out':\n",
    "            layer_out = tf.nn.softmax(layer_in)\n",
    "        else:\n",
    "            layer_out = tf.nn.relu(layer_in)\n",
    "            \n",
    "        # For histogram summart\n",
    "        tf.summary.histogram('weights',w)\n",
    "        tf.summary.histogram('biases',b)\n",
    "            \n",
    "        return layer_out       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# MODEL WITHOUT DROPOUT\\nlayer_1 = setup_layer(X, weight_dim = [TOTAL_INPUTS, n_hidden1], \\n                      bias_dim=[n_hidden1], name='layer_1')\\n\\nlayer_2 = setup_layer(layer_1, weight_dim = [n_hidden1, n_hidden2], \\n                      bias_dim=[n_hidden2], name='layer_2')\\n\\nlayer_3 = setup_layer(layer_2, weight_dim = [n_hidden2, n_hidden3], \\n                      bias_dim=[n_hidden3], name='layer_3')\\n\\noutput = setup_layer(layer_3, weight_dim = [n_hidden3, NR_CLASSES], \\n                      bias_dim=[NR_CLASSES], name='out')\\n\\nmodel_name = f'{n_hidden1}-{n_hidden2}-{n_hidden3} LR{learning_rate} E{nr_epochs}'\\n\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# MODEL WITHOUT DROPOUT\n",
    "layer_1 = setup_layer(X, weight_dim = [TOTAL_INPUTS, n_hidden1], \n",
    "                      bias_dim=[n_hidden1], name='layer_1')\n",
    "\n",
    "layer_2 = setup_layer(layer_1, weight_dim = [n_hidden1, n_hidden2], \n",
    "                      bias_dim=[n_hidden2], name='layer_2')\n",
    "\n",
    "layer_3 = setup_layer(layer_2, weight_dim = [n_hidden2, n_hidden3], \n",
    "                      bias_dim=[n_hidden3], name='layer_3')\n",
    "\n",
    "output = setup_layer(layer_3, weight_dim = [n_hidden3, NR_CLASSES], \n",
    "                      bias_dim=[NR_CLASSES], name='out')\n",
    "\n",
    "model_name = f'{n_hidden1}-{n_hidden2}-{n_hidden3} LR{learning_rate} E{nr_epochs}'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/faiqueali/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# MODEL WITH DROPOUT\n",
    "layer_1 = setup_layer(X, weight_dim = [TOTAL_INPUTS, n_hidden1], \n",
    "                      bias_dim=[n_hidden1], name='layer_1')\n",
    "\n",
    "layer_drop = tf.nn.dropout(layer_1, keep_prob=0.8, name='dropout_layer')\n",
    "\n",
    "layer_2 = setup_layer(layer_drop, weight_dim = [n_hidden1, n_hidden2], \n",
    "                      bias_dim=[n_hidden2], name='layer_2')\n",
    "\n",
    "layer_3 = setup_layer(layer_2, weight_dim = [n_hidden2, n_hidden3], \n",
    "                      bias_dim=[n_hidden3], name='layer_3')\n",
    "\n",
    "output = setup_layer(layer_3, weight_dim = [n_hidden3, NR_CLASSES], \n",
    "                      bias_dim=[NR_CLASSES], name='out')\n",
    "\n",
    "model_name = f'{n_hidden1}-{n_hidden2}-{n_hidden3} LR{learning_rate} E{nr_epochs}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created directories !!\n"
     ]
    }
   ],
   "source": [
    "# Folder for tensorboard\n",
    "\n",
    "folder_name = f'{model_name} at {strftime(\"%H:%M\")}'\n",
    "directory = os.path.join(LOGGING_PATH, folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as exception:\n",
    "    print(exception.strerror)\n",
    "else:\n",
    "    print('Successfully created directories !!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, Optimisation & Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Loss func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss_cal'):\n",
    "    # Computes softmax cross entropy between logits and labels\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    # Initialize the optimizer and specify 'adam' to use and define learning rate\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    # Specify operation what the optimizer will do to minimize the loss\n",
    "    train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy_calc'):\n",
    "    #argmax with extract the index of highest value(1) from each row\n",
    "    correct_pred = tf.equal(tf.argmax(output, axis=1), tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('performance'):\n",
    "    # Creating Tensorboard Summaries\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cost', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Input Images in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('show_image'):\n",
    "    # All data in placeholder\n",
    "    x_image = tf.reshape(X, [-1,28,28,1])\n",
    "    tf.summary.image('image_input', x_image, max_outputs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Filewriter and Merge Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter(directory + '/train')\n",
    "train_writer.add_graph(sess.graph)\n",
    "\n",
    "validation_writer = tf.summary.FileWriter(directory + '/validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old part code\n",
    "# b3.eval(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = y_train.shape[0]\n",
    "nr_iterations = int(num_examples / size_of_batch)\n",
    "\n",
    "# To track where a batch ends and other starts\n",
    "index_in_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    \n",
    "    # To acess global variables to get their current values\n",
    "    global num_examples\n",
    "    global index_in_epoch\n",
    "    \n",
    "    # Starting value and incrementing index\n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # To reset\n",
    "    if index_in_epoch > num_examples:\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "    \n",
    "    end = index_in_epoch\n",
    "    \n",
    "    # Returing all values(data) & features(labels) between starting and ending batch value\n",
    "    return data[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t| Training Accuracy = 0.6169999837875366\n",
      "Epoch 1 \t| Training Accuracy = 0.6240000128746033\n",
      "Epoch 2 \t| Training Accuracy = 0.7409999966621399\n",
      "Epoch 3 \t| Training Accuracy = 0.7730000019073486\n",
      "Epoch 4 \t| Training Accuracy = 0.7799999713897705\n",
      "Epoch 5 \t| Training Accuracy = 0.7940000295639038\n",
      "Epoch 6 \t| Training Accuracy = 0.8489999771118164\n",
      "Epoch 7 \t| Training Accuracy = 0.8389999866485596\n",
      "Epoch 8 \t| Training Accuracy = 0.8450000286102295\n",
      "Epoch 9 \t| Training Accuracy = 0.8460000157356262\n",
      "Epoch 10 \t| Training Accuracy = 0.8489999771118164\n",
      "Epoch 11 \t| Training Accuracy = 0.8500000238418579\n",
      "Epoch 12 \t| Training Accuracy = 0.8460000157356262\n",
      "Epoch 13 \t| Training Accuracy = 0.8500000238418579\n",
      "Epoch 14 \t| Training Accuracy = 0.8529999852180481\n",
      "Epoch 15 \t| Training Accuracy = 0.8539999723434448\n",
      "Epoch 16 \t| Training Accuracy = 0.8539999723434448\n",
      "Epoch 17 \t| Training Accuracy = 0.871999979019165\n",
      "Epoch 18 \t| Training Accuracy = 0.871999979019165\n",
      "Epoch 19 \t| Training Accuracy = 0.9390000104904175\n",
      "Epoch 20 \t| Training Accuracy = 0.9390000104904175\n",
      "Epoch 21 \t| Training Accuracy = 0.9350000023841858\n",
      "Epoch 22 \t| Training Accuracy = 0.9390000104904175\n",
      "Epoch 23 \t| Training Accuracy = 0.9369999766349792\n",
      "Epoch 24 \t| Training Accuracy = 0.9409999847412109\n",
      "Epoch 25 \t| Training Accuracy = 0.9380000233650208\n",
      "Epoch 26 \t| Training Accuracy = 0.9419999718666077\n",
      "Epoch 27 \t| Training Accuracy = 0.9430000185966492\n",
      "Epoch 28 \t| Training Accuracy = 0.9390000104904175\n",
      "Epoch 29 \t| Training Accuracy = 0.9419999718666077\n",
      "Epoch 30 \t| Training Accuracy = 0.9419999718666077\n",
      "Epoch 31 \t| Training Accuracy = 0.9399999976158142\n",
      "Epoch 32 \t| Training Accuracy = 0.9409999847412109\n",
      "Epoch 33 \t| Training Accuracy = 0.9620000123977661\n",
      "Epoch 34 \t| Training Accuracy = 0.9610000252723694\n",
      "Epoch 35 \t| Training Accuracy = 0.9610000252723694\n",
      "Epoch 36 \t| Training Accuracy = 0.9599999785423279\n",
      "Epoch 37 \t| Training Accuracy = 0.9649999737739563\n",
      "Epoch 38 \t| Training Accuracy = 0.9599999785423279\n",
      "Epoch 39 \t| Training Accuracy = 0.9620000123977661\n",
      "Epoch 40 \t| Training Accuracy = 0.9620000123977661\n",
      "Epoch 41 \t| Training Accuracy = 0.9629999995231628\n",
      "Epoch 42 \t| Training Accuracy = 0.9649999737739563\n",
      "Epoch 43 \t| Training Accuracy = 0.9649999737739563\n",
      "Epoch 44 \t| Training Accuracy = 0.9639999866485596\n",
      "Epoch 45 \t| Training Accuracy = 0.9599999785423279\n",
      "Epoch 46 \t| Training Accuracy = 0.9649999737739563\n",
      "Epoch 47 \t| Training Accuracy = 0.9620000123977661\n",
      "Epoch 48 \t| Training Accuracy = 0.968999981880188\n",
      "Epoch 49 \t| Training Accuracy = 0.9620000123977661\n",
      "Done training !!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nr_epochs):\n",
    "    \n",
    "    # ===================== TRAINING DATASET =====================\n",
    "    for i in range(nr_iterations):\n",
    "        # 1- getting batch data and features\n",
    "        batch_x, batch_y = next_batch(batch_size= size_of_batch, data=x_train, labels=y_train)\n",
    "        \n",
    "        # 2- To feed it to our session\n",
    "        feed_dictionary = {X:batch_x, Y:batch_y}\n",
    "        \n",
    "        # To do calculations\n",
    "        sess.run(train_step, feed_dict=feed_dictionary)\n",
    "        \n",
    "    # Fetching merged summary and accuracy from session\n",
    "    s, batch_accuracy = sess.run(fetches=[merged_summary, accuracy], feed_dict=feed_dictionary)\n",
    "    \n",
    "    # Writing summary to file\n",
    "    train_writer.add_summary(s, epoch)\n",
    "        \n",
    "    print(f'Epoch {epoch} \\t| Training Accuracy = {batch_accuracy}')\n",
    "    \n",
    "    \n",
    "    # ===================== VALIDATION =========================\n",
    "    summary = sess.run(fetches=merged_summary, feed_dict={X: x_val, Y: y_val})\n",
    "    validation_writer.add_summary(summary, epoch)\n",
    "    \n",
    "    \n",
    "print('Done training !!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_the_image(image_name):\n",
    "    '''\n",
    "    Input: Takes image name\n",
    "    Returns: Predicted Image\n",
    "    '''\n",
    "    # Opens Image\n",
    "    img = Image.open(image_name)\n",
    "    \n",
    "    # Converting image to black and white\n",
    "    bw = img.convert('L')\n",
    "    img_array = np.invert(bw)\n",
    "    \n",
    "    # To convert 2D(img_array) to flatten\n",
    "    test_img = img_array.ravel()\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = sess.run(fetches= tf.argmax(output, axis=1), feed_dict={X:[test_img]})\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABCUlEQVR4nOWVvRGDMAyFcSq1FNxRwgSswR40lGxA6REoYQuzGaVSkDOKZCuY0OTyOvzz6T3ZPgwiZnfrcTtRg67r2jRNURR1XS/LkkbFiMqy9GsAILYsqCiU1Z7n+Vuoc45Bk8yGoTS71zRN16HUZt/3AOA/x3G8CKU2EdFaS/2e4Qag1OY+sm1b27Z+3FqbBqXZ6Tjlfjw0DmXZGdf3VzfLd8rsVMMw7LPGmK7rTkFZdudcVVXybnnFXsQbNM9zBSEVa+4bNImomD2g8mnuCjbXnxgASO4BVc5dir4I2YRjv25N5zKzL2jszuuiTQhAk7JLsyxcxmqezK4rozVvISKiwZ//Rf8H9AkgI6JHCNIMeQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F98784AAD60>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name_1 = 'NIST/test_img1.png'\n",
    "img = Image.open(image_name_1)\n",
    "img  #To display the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test image is A\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction for test image is {ALPHABETS[predict_the_image(image_name_1)[0]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAAYUlEQVR4nO2UwRIAEAhErfH/v5yDW6JJOTTszcFrtQVEVKJVw4m3oI2dARxQWA9jnDIr3KlY2URcQk2aHeRJ/3GonP5+BdSBO3Gqbh3msuOO5/fKE9SHvgwVht+vPM/PA+2y6Bg4vsOTdgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F97144B3FA0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name_2 = 'NIST/test_img3.png'\n",
    "img = Image.open(image_name_2)\n",
    "img  #To display the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test image is F\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction for test image is {ALPHABETS[predict_the_image(image_name_2)[0]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAAl0lEQVR4nN3V0QqAIAwF0G70/7+8HqQRbs67FIvuUwgemy6DiGyzs08XWRTAZLSIAHg6Qi1Eukcg6nP2MH1UxWe94ZQ/KProoBihNnwDsH2qHOPWqJ3DjFTxT99uqI4wb9rs02CBbhZeKNkbpJ5uqyvifdxdI9gW6tvPxim/9QpypYuyLZXKe7+TdWhwkk5Ljedj5f8CPQFF7TNXJvoQZAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F987744A2B0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name_3 = 'NIST/test_img4.png'\n",
    "img = Image.open(image_name_3)\n",
    "img  #To display the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test image is G\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction for test image is {ALPHABETS[predict_the_image(image_name_3)[0]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAA80lEQVR4nM2VsQ2EMAxFnatcUlIyCmxAyQjZgpIRKMMWyWaUuQIJReHbQA7p+GW+/fgxIZgYIz2tz+PEl0FDCF3XLcuC7SjIe9+2rXMOunVdExEzQ1eEnrSpmfCq915p010RusWU2nRXhO491lrFlaAmosNvjNl7FFcqAEcqhHBchLLWYqNkZKoLZpq+WTjQEuhpzBLoacx5nsuhUsOlrdyCpjEfg6YxS6DH+ymLCWswlJkJqWmaqqqyRWaG3Bw6TROEphrHUeeCuejc7ailNUeuOOxU67oOw0BEfd/DZ2d3+SWopJ2bfSn46vtRb/pF/wH6BSPdPhuye00JAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F967B3E1B80>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name_4 = 'NIST/test_image7.png'\n",
    "img = Image.open(image_name_4)\n",
    "img  #To display the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test image is U\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction for test image is {ALPHABETS[predict_the_image(image_name_4)[0]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABAUlEQVR4nO2VMRKDIBBFIRWlpeVyC0uPYakdN/EIlHgLvNnakYKZzSZgwJjJZDL5lTOyT/5nV2QIQbxbl7cT/9CPQ9d11VrLe2mtl2UpUENO3nsAaJomW6KUstZmC6My0LZtY3HXdUIIYwy9mueZo51zVVBr7XMTiKiUIm4Z6r0ny8Mw7Lnj+y1DybgQAhH3oIFNdhlKS3mOZKLvewrxFWi6LpqgECnWU1CiAIBzjmI9AE17BRHHceT9VAst9go/91oorwGAbME0TcegfLOVihEXoKnBotJ53Z1FfiypjDF8zQNXhnMXn5SSuIgYn8/+pCmubdtuXzq506x+4476LugVQsTzbNJPTTcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F9711E2D670>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name_5 = 'NIST/test_image8.png'\n",
    "img = Image.open(image_name_5)\n",
    "img  #To display the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test image is Q\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction for test image is {ALPHABETS[predict_the_image(image_name_5)[0]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAACzUlEQVR4nNWVv0vzQBjHc5frz0SjNq2VUpFWpIJQobgUXFSkuvgHuPoH+AcUCuLm4OLq4Obm1sWlUFwEna2pJQWbiGjbYJteLiRxCG+pbWze98XF73b3PM+H5+6bewIsy6J+WvDHib8KilwzCoVCKpUqlUrLy8uHh4d/AwVjjAIADC5XVlYSicT8/Hw6nT44OPgHqCiKrVbr9PS0XC6LojgYWl9fl2WZEELTdCKRSKVSZ2dnjtDhO5Uk6eLiolKpdDode8f6o2w2Ozs76/F4er1erVarVCrfdToMVVW1Wq22Wi17eX193Q9tbGzs7OywLMswjGma/ZxRfTFKlmUAAMa42WwihBYXF3me70e3t7chhKqqXl1dKYpCCGk0GrFYzKVTXdfv7u40TTNNE0IYCoVWV1cHE7a2tgghHMcBAHRdPz8/dz9+KBRSVdXn8yGEKIoKBoOjBScnJ+l02uv10jRdr9fdoQzDvL+/0zQNAGBZdmZmxrEml8shhAghb29vjtxhoyKRiGmaFEVhjL1eryM0k8ns7e2xLPv09FQqlUYTvhhVLBaDwSAhZHp6mmXZ7/ylaRpC6PF4/H7/wsKCS6e7u7v2bSKEdF2PRCKOUI7jGIbheV7TNFmWHx8fx0EpigqHwxzHQQgxxkMvqi+GYQRBMAzDpvv9fhfoxMSEoijNZpMQQgi5ublx5AYCAV3XA4GAz+fTNM0FGo1Gl5aWOI4zDOPj40MQBEdot9s1DMM0TVEUHx4eXKBra2s8z8/NzUEI2+22JEkYY0mSut3uy8tLtVq9v7/f398XBAFj3Ov1pqamksnkEMR59OXz+dvb23q93m63w+Gwfd7X11fLsiCEAABCyOTkZKFQ2NzctBMG5Tykj4+Pj46ODMPAGCuK8vz8bO/H4/H+C47H49FodJRIUQOTzVGjH8Dl5WW5XB5fNW7y/7d+z9/090A/ARbqgdLrVO1ZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F9711E0F790>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name_6 = 'NIST/test_image9.png'\n",
    "img = Image.open(image_name_6)\n",
    "img  #To display the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test image is O\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction for test image is {ALPHABETS[predict_the_image(image_name_6)[0]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABLklEQVR4nN2Vv42FMAzGwxXIJSVlMgFrMELKlJRsQJkRQke28ErZwGWuQBfl5R9w70kn3VcSfz/Zxk467z37tL4+TvxTqLVWCNH9SAhhrW0ZfFOIyDnPXQDQcLWgiNj3fS0brfVjKCIOwxAQUkoi8t6v6xqSPY7jGXQcx0BcliV8JyIAaHOr0CLxlNa63dxraPFUKdUIKHsQsebJ5+EuNG5ojVVrThVaG6M7xDLUGPNrXBUa1/6IFZTu/r7vzrkiEQCmabrVl1qa5wrFo3Nq27bLTFNoMJ9LeSqedsYY57y2oAVo/IuSOCKa5/km+sUc156H5t0AAGPMBbRYe6KkFax0B75Aw/VTIxbRAJAkwfLQm8NIRFJKxphSKjnq/HtPtHMuX5Z3oUX9s3f/qb4BxAMPLCdcpyEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F9711D59C40>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name_7 = 'NIST/test_image10.png'\n",
    "img = Image.open(image_name_7)\n",
    "img  #To display the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test image is R\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction for test image is {ALPHABETS[predict_the_image(image_name_7)[0]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAAlUlEQVR4nO2VwQ7AIAhD6bL//2V2M4ZA7dQdTOZNhWcxUuHutntc24lmdvNtAP1ULAtVXMC9QufQntgHtHXOZXfq7iG5TUkdOZQnKNxSKSlwyI1QLlM5MoEqOUMR84+fXMJSR1UF7WnTIHYVmor9xFBOh4otoEJFy+DHJ0pFLoDKCWdMuhLBlFahaUzu8cf8pj90/3gAq3lRHjaX7A4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F97115DA790>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name_8 = 'NIST/test_img2.png'\n",
    "img = Image.open(image_name_8)\n",
    "img  #To display the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test image is U\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction for test image is {ALPHABETS[predict_the_image(image_name_8)[0]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAAWUlEQVR4nGP8//8/A7UBE9VNHDWUBVOIkZERjwZiIhbdpfhNJEYBAwMDI0GbIaaQlPKGTkSNGjpq6Ig1FF8phTW/45eFqkGTI6YQItlQYkwnXLCN1qYj2FAA6ishL+d+z+EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F9711EBE0A0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name_9 = 'NIST/test_img6.png'\n",
    "img = Image.open(image_name_9)\n",
    "img  #To display the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test image is I\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction for test image is {ALPHABETS[predict_the_image(image_name_9)[0]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge:** Calculate the accuracy over the test dataset(`x_test` and `y_test`). Use your knowledge of running a session to get the accuracy. Display the accuracy as a percentage rounded to two decimal numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is 94.90%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = sess.run(fetches=accuracy, feed_dict={X:x_test, Y:y_test})\n",
    "print(f'Accuracy on test set is {test_accuracy:0.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset for the Next Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer.close()\n",
    "validation_writer.close()\n",
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
