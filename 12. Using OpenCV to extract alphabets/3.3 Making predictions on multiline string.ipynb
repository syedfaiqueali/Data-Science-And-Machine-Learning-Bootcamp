{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.saved_model import load, tag_constants\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_IMAGE = 'Image_Samples/Sample3.jpeg'\n",
    "\n",
    "ALPHABETS = {\n",
    "    0:'A', 1:'B', 2:'C', 3:'D', 4:'E', 5:'F', 6:'G', 7:'H', 8:'I',9:'J',\n",
    "    10:'K', 11:'L', 12:'M', 13:'N', 14:'O', 15:'P', 16:'Q', 17:'R', 18:'S',\n",
    "    19:'T', 20:'U', 21:'V', 22:'W', 23:'X', 24: 'Y', 25:'Z'\n",
    "}\n",
    "\n",
    "ARR_FINAL_WORDS = []\n",
    "ARR_FINAL_ROWS = []\n",
    "\n",
    "ARR_FINAL_PREDICTION_ROWS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_childs_of_parent(parent, hierarchy):\n",
    "    '''\n",
    "    This function will return all the child of that contour\n",
    "    Parameters:\n",
    "    parent : Int value\n",
    "    hierarchy: contours hierarchy\n",
    "    '''\n",
    "    # Initialize\n",
    "    hier_len = hierarchy.shape[1] #Total contours\n",
    "    child_contours = {} #Empty Dict\n",
    "    \n",
    "    for x in range(hier_len):\n",
    "        # To loop through whole hierarchy\n",
    "        next_contour, pre_contour, first_child, parent_contour = hierarchy[0][x]\n",
    "        \n",
    "        # If desired parent found, so append it all childs\n",
    "        if parent == parent_contour:\n",
    "            child_contours[x] = hierarchy[0][x]\n",
    "        \n",
    "    # return all the found contours\n",
    "    return child_contours\n",
    "\n",
    "def sort_contours(contours, method=\"left-to-right\"):\n",
    "    '''\n",
    "    This function will sort contours\n",
    "    Parameters:\n",
    "    contours: All contours\n",
    "    method: String\n",
    "    ''' \n",
    "    # Initialize the reverse flag and sort index\n",
    "    reverse = False\n",
    "    i=0\n",
    "    \n",
    "    # Handle if we need to sort in reverse\n",
    "    if method == \"right-to-left\" or method == \"bottom-to-top\":\n",
    "        reverse = True\n",
    "        \n",
    "    # Handle if we need are sorting against the y-cord rather than \n",
    "    # the x-cord of the bounding box\n",
    "    if method == \"top-to-bottom\" or method == \"bottom-to-top\":\n",
    "        i = 1\n",
    "        \n",
    "    # construct the list of bounding boxes and sort them from from top\n",
    "    # to bottom\n",
    "    boundingBoxes = [cv2.boundingRect(c) for c in contours]\n",
    "    (contours, boundingBoxes) = zip(*sorted(zip(contours,boundingBoxes),key=lambda b:b[1][i], reverse=reverse))\n",
    "    \n",
    "    # return the list of sorted contours and bounding boxes\n",
    "    return (contours, boundingBoxes)\n",
    "\n",
    "\n",
    "def extract_desired_contours(contours_dict, contours):\n",
    "    '''\n",
    "    This function will extract only desired contours\n",
    "    Parameters:\n",
    "    contours_dict : Desired contours as Dict\n",
    "    contours: All contours\n",
    "    ''' \n",
    "    desired_contours = []\n",
    "    \n",
    "    for i in range(len(contours)):\n",
    "        for key,value in contours_dict.items():\n",
    "            next_contour, pre_contour, first_child, parent = value\n",
    "            if i == first_child:\n",
    "                desired_contours.append(contours[i])\n",
    "                \n",
    "    return desired_contours\n",
    "\n",
    "\n",
    "def draw_rectangle_on_img_contours_and_croppping_them(contours, image, arr_contours):\n",
    "    '''\n",
    "    This function will draw rectangles on image\n",
    "    Parameters:\n",
    "    contours : array\n",
    "    image: image\n",
    "    arr_contours: array in which cropped contours to be saved\n",
    "    '''\n",
    "    total_contours = len(contours)\n",
    "    rect_color = (255,0,0)\n",
    "    rect_stroke_width = 2\n",
    "    \n",
    "    new_img = image.copy()\n",
    "    \n",
    "    for i in range(total_contours):\n",
    "        cnt = contours[i]\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        image = cv2.rectangle(image, (x,y), (x+w,y+h), rect_color, rect_stroke_width)\n",
    "        \n",
    "        # Step 5: Cropping individual contours and saving them in array\n",
    "        cropping_rectangles_and_saving_them(x,y,x+w,y+h,new_img,i,arr_contours)\n",
    "        \n",
    "    # To visualize all contours on image\n",
    "    #plt.imshow(image)\n",
    "    #plt.show()\n",
    "      \n",
    "\n",
    "def cropping_rectangles_and_saving_them(left,upper,right,lower, image_name, cropped_image_name, arr_contours):\n",
    "    '''\n",
    "    This function will crop all the bounding rectangles from an image\n",
    "    Parameters:\n",
    "    left=x,upper=y,right=x+w,lower=y+h\n",
    "    image_name: original image\n",
    "    cropped_image_name: individual cropped images name\n",
    "    '''\n",
    "    # Opens image using PIL\n",
    "    im = Image.fromarray(image_name)\n",
    "    \n",
    "    # Crop image from original image\n",
    "    box = (left,upper,right,lower)\n",
    "    cropped_image = im.crop(box)\n",
    "    \n",
    "    # Appending cropped contours\n",
    "    arr_contours.append(cropped_image)\n",
    "    \n",
    "    \n",
    "def resizing_cropped_img_to_20_by_20(arr_cropped_images):\n",
    "    '''\n",
    "    This function will resize all the cropped images to 20x20\n",
    "    Parameters:\n",
    "    arr_cropped_images: arr containing all cropped numbers\n",
    "    '''\n",
    "    new_Arr = []\n",
    "    \n",
    "    for i in range(len(arr_cropped_images)):\n",
    "        # Step:6 Resize img to 20 by 20\n",
    "        new_Arr.append(arr_cropped_images[i].resize((20,20)))\n",
    "        \n",
    "    return new_Arr\n",
    "    \n",
    "    \n",
    "def add_borders_to_img(arr_20x20_imgs):\n",
    "    '''\n",
    "    This function will resize all the cropped images to 28x28\n",
    "    Parameters:\n",
    "    arr_20x20_imgs: arr containing all 20x20 images\n",
    "    '''\n",
    "    new_Arr = []\n",
    "    \n",
    "    for i in range(len(arr_20x20_imgs)):\n",
    "        old_size = arr_20x20_imgs[i].size\n",
    "        new_size = (28,28)\n",
    "        \n",
    "        img_28x28 = Image.new('RGB',new_size, color=(255,255,255))\n",
    "        img_28x28.paste(arr_20x20_imgs[i],((new_size[0]-old_size[0])//2,(new_size[1]-old_size[1])//2))\n",
    "        new_Arr.append(img_28x28)\n",
    "        \n",
    "    return new_Arr \n",
    "\n",
    "def ret_x_cord_contour(contours):\n",
    "    '''\n",
    "    This func will get x-cord for contour\n",
    "    Parameters:\n",
    "    contours: Array containing all contours\n",
    "    '''\n",
    "    if int(cv2.contourArea(contours)) > 0:\n",
    "        cent_moment = cv2.moments(contours)\n",
    "        return cent_moment['m10']/cent_moment['m00']\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def get_contours_of_the_image(image, dilation=False):\n",
    "    '''\n",
    "    This func will find contours on image\n",
    "    And return them in sorted order\n",
    "    Parameters:\n",
    "    image: Image \n",
    "    dilation: Boolean value\n",
    "    '''\n",
    "    # Convert binary to RGB\n",
    "    img = image.convert('RGB')\n",
    "    img = np.array(img)\n",
    "\n",
    "    # convert the image to grayscale format\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # apply thresholding\n",
    "    ret, thresh = cv2.threshold(img_gray, 150, 255, cv2.THRESH_OTSU|cv2.THRESH_BINARY)\n",
    "\n",
    "    if dilation:\n",
    "        # Finding columns(words)\n",
    "        rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (30,1))\n",
    "        dilation = cv2.dilate(thresh, rect_kernel, iterations=1)\n",
    "\n",
    "        # detect the contours on the binary image using cv2.CHAIN_APPROX_NONE\n",
    "        contours, hierarchy = cv2.findContours(image=dilation, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE)\n",
    "    else:\n",
    "        # detect the contours on the binary image using cv2.CHAIN_APPROX_NONE\n",
    "        contours, hierarchy = cv2.findContours(image=thresh, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE)\n",
    "        \n",
    "    # Sort contours from left->Right\n",
    "    contours = sorted(contours, key=ret_x_cord_contour, reverse=False)\n",
    "    \n",
    "    return contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image using opencv\n",
    "img = cv2.imread(SAMPLE_IMAGE)\n",
    "\n",
    "# convert the image to grayscale format\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# apply thresholding\n",
    "ret, thresh = cv2.threshold(img_gray, 150, 255, cv2.THRESH_OTSU|cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# ----------------------------------\n",
    "# 1. FINDING TOTAL ROWS\n",
    "# ----------------------------------\n",
    "rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (120,10))\n",
    "dilation = cv2.dilate(thresh, rect_kernel, iterations=1)\n",
    "\n",
    "# detect the contours on the binary image using cv2.CHAIN_APPROX_NONE\n",
    "contours, hierarchy = cv2.findContours(image=dilation, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE)\n",
    " \n",
    "if len(contours) > 0:\n",
    "    # start from 1st row\n",
    "    #contours.reverse()\n",
    "    \n",
    "    # Crop all rows\n",
    "    ARR_CROPPED_ROWS = []\n",
    "    draw_rectangle_on_img_contours_and_croppping_them(contours, thresh.copy(),ARR_CROPPED_ROWS)\n",
    "    \n",
    "    # ------------------------------------\n",
    "    # 2. FINDING COLS(WORDS) IN EVERY ROW\n",
    "    # ------------------------------------\n",
    "    \n",
    "    if len(ARR_CROPPED_ROWS) > 0:\n",
    "        for i in range(len(ARR_CROPPED_ROWS)):\n",
    "            # Convert binary to RGB\n",
    "            img = ARR_CROPPED_ROWS[i].convert('RGB')\n",
    "            img = np.array(img)\n",
    "            \n",
    "            # convert the image to grayscale format\n",
    "            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # apply thresholding\n",
    "            ret, thresh = cv2.threshold(img_gray, 150, 255, cv2.THRESH_OTSU|cv2.THRESH_BINARY)\n",
    "            \n",
    "            # Finding columns(words)\n",
    "            rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (50,1))\n",
    "            dilation = cv2.dilate(thresh, rect_kernel, iterations=1)\n",
    "            \n",
    "            # detect the contours on the binary image using cv2.CHAIN_APPROX_NONE\n",
    "            contours, hierarchy = cv2.findContours(image=dilation, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE)\n",
    "            \n",
    "            # Sort contours from left->Right\n",
    "            contours = sorted(contours, key=ret_x_cord_contour, reverse=False)\n",
    "            \n",
    "            # Crop all words\n",
    "            ARR_CROPPED_WORDS = []\n",
    "            draw_rectangle_on_img_contours_and_croppping_them(contours, thresh.copy(),ARR_CROPPED_WORDS)\n",
    "\n",
    "            # ------------------------------------\n",
    "            # 3. FINDING CHARS IN EVERY WORD\n",
    "            # ------------------------------------\n",
    "            ARR_FINAL_WORDS = []\n",
    "            \n",
    "            if len(ARR_CROPPED_WORDS) > 0:\n",
    "                for j in range(len(ARR_CROPPED_WORDS)):\n",
    "                    # Read image using opencv\n",
    "                    img = ARR_CROPPED_WORDS[j].convert('RGB')\n",
    "                    img = np.array(img)\n",
    "\n",
    "                    # convert the image to grayscale format\n",
    "                    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    # apply thresholding\n",
    "                    ret, thresh = cv2.threshold(img_gray, 150, 255, cv2.THRESH_OTSU|cv2.THRESH_BINARY)\n",
    "\n",
    "                    # detect the contours on the binary image using cv2.CHAIN_APPROX_NONE\n",
    "                    contours, hierarchy = cv2.findContours(image=thresh, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE)\n",
    "           \n",
    "                    # Sort contours from left->Right\n",
    "                    contours = sorted(contours, key=ret_x_cord_contour, reverse=False)\n",
    "                \n",
    "                    # Crop all chars\n",
    "                    ARR_CROPPED_CHARS = []\n",
    "                    draw_rectangle_on_img_contours_and_croppping_them(contours, thresh.copy(),ARR_CROPPED_CHARS)\n",
    "                    \n",
    "                    # \n",
    "                    ARR_CROPPED_CHARS_NEW = []\n",
    "                    for img in ARR_CROPPED_CHARS:\n",
    "                        img = np.array(img)\n",
    "                        img = cv2.bitwise_not(img)\n",
    "                        img = Image.fromarray(img)\n",
    "                        ARR_CROPPED_CHARS_NEW.append(img)\n",
    "                        \n",
    "                    \n",
    "                    # ------------------------------------------------\n",
    "                    # STEP 3.1: CONVERT CROPPED DIGITS TO 20X20\n",
    "                    # ------------------------------------------------\n",
    "                    arr_img_20x20 = resizing_cropped_img_to_20_by_20(ARR_CROPPED_CHARS_NEW)\n",
    "\n",
    "                    # ------------------------------------------------\n",
    "                    # STEP 3.2: CONVERT 20x20 TO 28X28\n",
    "                    # ------------------------------------------------\n",
    "                    arr_img_28x28 = add_borders_to_img(arr_img_20x20)\n",
    "                    \n",
    "                    # ------------------------------------\n",
    "                    # 4. APPENDING CHARS TO WORDS ARRAY\n",
    "                    # ------------------------------------\n",
    "                    ARR_FINAL_WORDS.append(arr_img_28x28)\n",
    "                    \n",
    "                # ------------------------------------\n",
    "                # 5. APPENDING WORDS ARRAY TO ROWS ARRAY\n",
    "                # ------------------------------------\n",
    "                ARR_FINAL_ROWS.append(ARR_FINAL_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAANoklEQVR4nO3db6hc9Z3H8c9HrRo1YNx7I0kaNrXkwYbA2maIolKiRVFBomClEUNE5faBEQsFV9wHDfjAIKaiYRHjGptdXEshCYrEWokVCWhx/LOaeN2NK7HeeElu/EOtBNzodx/ck+Ua75y5nvlzJn7fLxhm5nznzPlyks89M/M7Mz9HhAB8951QdwMA+oOwA0kQdiAJwg4kQdiBJE7q58aGhoZi0aJF/dwkkMq+fft06NAhT1frKOy2L5f0gKQTJf1rRKwve/yiRYvUbDY72SSAEo1Go2Wt8st42ydK+hdJV0haImmV7SVVnw9Ab3Xynn25pHcj4r2I+ELS7ySt7E5bALqtk7AvkPTBlPtjxbKvsT1iu2m7OTEx0cHmAHSik7BP9yHAN869jYhNEdGIiMbw8HAHmwPQiU7CPiZp4ZT735f0YWftAOiVTsL+iqTFtn9g+2RJP5f0VHfaAtBtlYfeIuKI7bWSntXk0NvmiNjTtc4AdFVH4+wRsUPSji71AqCHOF0WSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgST6OmVznV566aXS+qpVq0rr77//fjfbSePGG29sWXvsscf61wg4sgNZEHYgCcIOJEHYgSQIO5AEYQeSIOxAEmnG2W+66abSertx9KGhoZa1pUuXVuopg/PPP7/uFlDoKOy290n6TNKXko5ERKMbTQHovm4c2S+OiENdeB4APcR7diCJTsMekv5o+1XbI9M9wPaI7abt5sTERIebA1BVp2G/MCJ+LOkKSbfa/smxD4iITRHRiIjG8PBwh5sDUFVHYY+ID4vrg5K2S1rejaYAdF/lsNs+3fbso7clXSZpd7caA9BdnXwaf7ak7baPPs9/RMQfutJVD7zzzjsdrf/QQw+1rF177bUdPTfQD5XDHhHvSfrHLvYCoIcYegOSIOxAEoQdSIKwA0kQdiCJNF9x7dScOXPqbgHoCEd2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCcfYZOukkdhWObxzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJBo9n6N57721ZW7ZsWem6Z5xxRrfbAb41juxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kESacfbbbruttL5x48bS+o4dO1rWrrrqqtJ116xZU1ofGhoqrV9wwQWl9RNOqP43+8wzz6y8Lo4vbf+X2N5s+6Dt3VOWnWX7Odt7i2tmUAAG3EwOCb+VdPkxy+6UtDMiFkvaWdwHMMDahj0iXpT08TGLV0raUtzeIunq7rYFoNuqvtk7OyLGJam4ntvqgbZHbDdtNycmJipuDkCnev5pfERsiohGRDSGh4d7vTkALVQN+wHb8ySpuD7YvZYA9ELVsD8l6eh40hpJT3anHQC90nac3fYTklZIGrI9JunXktZL+r3tmyX9RdLPetlkN2zYsKGj9bdt29ay9sILL5Su267ezuzZs0vrtlvW2v3e/cUXX1xanzVrVmm9nfnz57esrVixonTdhQsXltYXL15cWj/llFNK69m0DXtErGpR+mmXewHQQ5wuCyRB2IEkCDuQBGEHkiDsQBKOiL5trNFoRLPZ7Nv2uumjjz5qWXvmmWdK1213mvDTTz9dWj98+HBpvcyhQ4dK63v37q383L3W7qu755xzTmn9uuuua1m7/PJjv9v1de1+Hvy0004rrdel0Wio2WxOOxbLkR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmCc/Tvu888/L62PjY119Pyffvppaf35559vWfvqq69K1y37+W5JGh0dLa1/8sknpfUyl1xySWl969atpfW6fqKbcXYAhB3IgrADSRB2IAnCDiRB2IEkCDuQBOPsOG7t37+/tP7www+3rN19992l65b9PLckvfzyy6X15cuXl9Z7hXF2AIQdyIKwA0kQdiAJwg4kQdiBJAg7kETbWVyBQbVgwYLS+tq1a1vWNm/eXLpuuzH8xx9/vLRe1zh7mbZHdtubbR+0vXvKsnW299t+o7hc2ds2AXRqJi/jfytpuukz7o+Ic4tL+U+KAKhd27BHxIuSPu5DLwB6qJMP6NbafrN4mT+n1YNsj9hu2m62m/MMQO9UDftDkn4o6VxJ45I2tHpgRGyKiEZENIaHhytuDkCnKoU9Ig5ExJcR8ZWkRyQN3kePAL6mUthtz5ty9xpJu1s9FsBgaDvObvsJSSskDdkek/RrSStsnyspJO2T9IvetQhUM3fu3Ja18847r3Tdbdu2lda3b99eWn/ggQdK63VoG/aIWDXN4kd70AuAHuJ0WSAJwg4kQdiBJAg7kARhB5LgK65IaWRkpLTebujtgw8+6GY7fcGRHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJwdKT377LN1t9B3HNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2XHc+uKLL0rr69evb1nbuHFjR9u+4447Olq/DhzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtkLhw8fLq0/8sgjLWunnnpq6bq33HJLaf2EE/ibW8Vdd91VWt+wYUPl5273b3bPPfdUfu66tP1fZnuh7T/ZHrW9x/btxfKzbD9ne29xPaf37QKoaiaHlCOSfhUR/yDpfEm32l4i6U5JOyNisaSdxX0AA6pt2CNiPCJeK25/JmlU0gJJKyVtKR62RdLVPeoRQBd8qzeLthdJ+pGkP0s6OyLGpck/CJLmtlhnxHbTdnNiYqLDdgFUNeOw2z5D0lZJv4yIv850vYjYFBGNiGgMDw9X6RFAF8wo7La/p8mgPx4RR6e3PGB7XlGfJ+lgb1oE0A1th95sW9KjkkYj4jdTSk9JWiNpfXH9ZE867JPR0dHS+u233175uZcsWVJav+iiiyo/96AbHx9vWXv77bdL1123bl1pfdeuXVVakiRdeumlpfUHH3ywtH48DpfOZJz9QkmrJb1l+41i2V2aDPnvbd8s6S+SftaTDgF0RduwR8QuSW5R/ml32wHQK8ffaxEAlRB2IAnCDiRB2IEkCDuQBF9xLZx88sml9ZNOar2rjhw5UrruZZddVlpftmxZ5W0Puj179rSsdXr69OzZs0vr11xzTcva/fffX7rurFmzKvU0yDiyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASx+8Abpe1+875li1bWtbuu+++0nVff/310non38s+ns2fP7+0vnr16tL6DTfcUFpfunTpt+7pu4wjO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwTh7od3vgF9//fWVasCg4MgOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0m0Dbvthbb/ZHvU9h7btxfL19neb/uN4nJl79sFUNVMTqo5IulXEfGa7dmSXrX9XFG7PyLKf7kBwECYyfzs45LGi9uf2R6VtKDXjQHorm/1nt32Ikk/kvTnYtFa22/a3mx7Tot1Rmw3bTc7ne4HQHUzDrvtMyRtlfTLiPirpIck/VDSuZo88m+Ybr2I2BQRjYhoDA8Pd94xgEpmFHbb39Nk0B+PiG2SFBEHIuLLiPhK0iOSlveuTQCdmsmn8Zb0qKTRiPjNlOXzpjzsGkm7u98egG6ZyafxF0paLekt228Uy+6StMr2uZJC0j5Jv+hBfwC6ZCafxu+S5GlKO7rfDoBe4Qw6IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEo6I/m3MnpD0/pRFQ5IO9a2Bb2dQexvUviR6q6qbvf19REz7+299Dfs3Nm43I6JRWwMlBrW3Qe1Loreq+tUbL+OBJAg7kETdYd9U8/bLDGpvg9qXRG9V9aW3Wt+zA+ifuo/sAPqEsANJ1BJ225fb/i/b79q+s44eWrG9z/ZbxTTUzZp72Wz7oO3dU5adZfs523uL62nn2Kupt4GYxrtkmvFa913d05/3/T277RMl/bekSyWNSXpF0qqIeLuvjbRge5+kRkTUfgKG7Z9I+pukf4uIpcWyeyV9HBHriz+UcyLinwakt3WS/lb3NN7FbEXzpk4zLulqSTeqxn1X0td16sN+q+PIvlzSuxHxXkR8Iel3klbW0MfAi4gXJX18zOKVkrYUt7do8j9L37XobSBExHhEvFbc/kzS0WnGa913JX31RR1hXyDpgyn3xzRY872HpD/aftX2SN3NTOPsiBiXJv/zSJpbcz/HajuNdz8dM834wOy7KtOfd6qOsE83ldQgjf9dGBE/lnSFpFuLl6uYmRlN490v00wzPhCqTn/eqTrCPiZp4ZT735f0YQ19TCsiPiyuD0rarsGbivrA0Rl0i+uDNffz/wZpGu/pphnXAOy7Oqc/ryPsr0habPsHtk+W9HNJT9XQxzfYPr344ES2T5d0mQZvKuqnJK0pbq+R9GSNvXzNoEzj3WqacdW872qf/jwi+n6RdKUmP5H/H0n/XEcPLfo6R9J/Fpc9dfcm6QlNvqz7X02+IrpZ0t9J2ilpb3F91gD19u+S3pL0piaDNa+m3i7S5FvDNyW9UVyurHvflfTVl/3G6bJAEpxBByRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ/B/AfxdAqpkZlAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = ARR_FINAL_ROWS[3][0][0]  #word = row,cols ;returns chars array of that word\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-884470e6aa96>:29: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n"
     ]
    }
   ],
   "source": [
    "ARR_FINAL_PREDICTION = []\n",
    "for row in range(len(ARR_FINAL_ROWS)):\n",
    "    \n",
    "    ARR_FINAL_PREDICTION_ROWS = []\n",
    "    for col in range(len(ARR_FINAL_ROWS[row])):\n",
    "        \n",
    "        ARR_FINAL_PREDICTION_WORDS = []\n",
    "        for char in range(len(ARR_FINAL_ROWS[row][col])):\n",
    "            # ------------------------------------------------\n",
    "            # STEP 6: CONVERT 28x28 TO FLAT ARRAY\n",
    "            # ------------------------------------------------\n",
    "            x_test = []\n",
    "            \n",
    "            bw = ARR_FINAL_ROWS[row][col][char].convert('L')\n",
    "            img_array = np.invert(bw)\n",
    "            test_img = img_array.ravel()\n",
    "            x_test.append(test_img)\n",
    "            \n",
    "            # ------------------------------------------------\n",
    "            # STEP 7: LOAD MODEL AND CREATE SESSION\n",
    "            # ------------------------------------------------\n",
    "            # Create a graph obj placeholder\n",
    "            graph = tf.Graph()\n",
    "\n",
    "            # Creating a sess obj and linking session and the graph\n",
    "            sess = tf.compat.v1.Session(graph=graph)\n",
    "\n",
    "            # Loading the Model\n",
    "            load(sess=sess, tags=[tag_constants.SERVING], export_dir='SavedModel')\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # STEP 8: MAKING PREDICTIONS\n",
    "            # ------------------------------------------------\n",
    "            # Return the `Tensor` with the given `name` and index=0 result\n",
    "            X = graph.get_tensor_by_name('X:0')\n",
    "\n",
    "            # Get hold of the tensor that will hold the predictions from the graph. Store these under y_pred\n",
    "            y_pred = graph.get_tensor_by_name('accuracy_calc/prediction:0')\n",
    "\n",
    "            # fetches = y_pred(the output we are after)\n",
    "            prediction = sess.run(fetches=y_pred, feed_dict={X: x_test})\n",
    "            \n",
    "            # ------------------------------------\n",
    "            # 9. APPENDING CHARS TO WORDS ARRAY\n",
    "            # ------------------------------------\n",
    "            ARR_FINAL_PREDICTION_WORDS.append(prediction)\n",
    "            \n",
    "        ARR_FINAL_PREDICTION_ROWS.append(ARR_FINAL_PREDICTION_WORDS)\n",
    "        \n",
    "    ARR_FINAL_PREDICTION.append(ARR_FINAL_PREDICTION_ROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a7e06194d139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARR_FINAL_PREDICTION\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "len(ARR_FINAL_PREDICTION[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# 10. DECODING ALHPABETS AND CONCATENATIING\n",
    "#     THEM INTO STRING\n",
    "# -----------------------------------------\n",
    "result = ''\n",
    "for row in range(len(ARR_FINAL_PREDICTION)):\n",
    "    for col in range(len(ARR_FINAL_PREDICTION[row])):\n",
    "        for char in range(len(ARR_FINAL_PREDICTION[row][col])):\n",
    "            result = result + ALPHABETS[ARR_FINAL_PREDICTION[row][col][char][0]]\n",
    "            \n",
    "        result = result + ' '\n",
    "        \n",
    "    result = result + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
