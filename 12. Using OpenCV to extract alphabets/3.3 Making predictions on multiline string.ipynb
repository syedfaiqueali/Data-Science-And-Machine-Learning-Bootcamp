{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.saved_model import load, tag_constants\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_IMAGE = 'Image_Samples/Sample3.jpeg'\n",
    "\n",
    "ALPHABETS = {\n",
    "    0:'A', 1:'B', 2:'C', 3:'D', 4:'E', 5:'F', 6:'G', 7:'H', 8:'I',9:'J',\n",
    "    10:'K', 11:'L', 12:'M', 13:'N', 14:'O', 15:'P', 16:'Q', 17:'R', 18:'S',\n",
    "    19:'T', 20:'U', 21:'V', 22:'W', 23:'X', 24: 'Y', 25:'Z'\n",
    "}\n",
    "\n",
    "ARR_FINAL_WORDS = []\n",
    "ARR_FINAL_ROWS = []\n",
    "\n",
    "ARR_FINAL_PREDICTION_ROWS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_childs_of_parent(parent, hierarchy):\n",
    "    '''\n",
    "    This function will return all the child of that contour\n",
    "    Parameters:\n",
    "    parent : Int value\n",
    "    hierarchy: contours hierarchy\n",
    "    '''\n",
    "    # Initialize\n",
    "    hier_len = hierarchy.shape[1] #Total contours\n",
    "    child_contours = {} #Empty Dict\n",
    "    \n",
    "    for x in range(hier_len):\n",
    "        # To loop through whole hierarchy\n",
    "        next_contour, pre_contour, first_child, parent_contour = hierarchy[0][x]\n",
    "        \n",
    "        # If desired parent found, so append it all childs\n",
    "        if parent == parent_contour:\n",
    "            child_contours[x] = hierarchy[0][x]\n",
    "        \n",
    "    # return all the found contours\n",
    "    return child_contours\n",
    "\n",
    "def sort_contours(contours, method=\"left-to-right\"):\n",
    "    '''\n",
    "    This function will sort contours\n",
    "    Parameters:\n",
    "    contours: All contours\n",
    "    method: String\n",
    "    ''' \n",
    "    # Initialize the reverse flag and sort index\n",
    "    reverse = False\n",
    "    i=0\n",
    "    \n",
    "    # Handle if we need to sort in reverse\n",
    "    if method == \"right-to-left\" or method == \"bottom-to-top\":\n",
    "        reverse = True\n",
    "        \n",
    "    # Handle if we need are sorting against the y-cord rather than \n",
    "    # the x-cord of the bounding box\n",
    "    if method == \"top-to-bottom\" or method == \"bottom-to-top\":\n",
    "        i = 1\n",
    "        \n",
    "    # construct the list of bounding boxes and sort them from from top\n",
    "    # to bottom\n",
    "    boundingBoxes = [cv2.boundingRect(c) for c in contours]\n",
    "    (contours, boundingBoxes) = zip(*sorted(zip(contours,boundingBoxes),key=lambda b:b[1][i], reverse=reverse))\n",
    "    \n",
    "    # return the list of sorted contours and bounding boxes\n",
    "    return (contours, boundingBoxes)\n",
    "\n",
    "\n",
    "def extract_desired_contours(contours_dict, contours):\n",
    "    '''\n",
    "    This function will extract only desired contours\n",
    "    Parameters:\n",
    "    contours_dict : Desired contours as Dict\n",
    "    contours: All contours\n",
    "    ''' \n",
    "    desired_contours = []\n",
    "    \n",
    "    for i in range(len(contours)):\n",
    "        for key,value in contours_dict.items():\n",
    "            next_contour, pre_contour, first_child, parent = value\n",
    "            if i == first_child:\n",
    "                desired_contours.append(contours[i])\n",
    "                \n",
    "    return desired_contours\n",
    "\n",
    "\n",
    "def draw_rectangle_on_img_contours_and_croppping_them(contours, image, arr_contours):\n",
    "    '''\n",
    "    This function will draw rectangles on image\n",
    "    Parameters:\n",
    "    contours : array\n",
    "    image: image\n",
    "    arr_contours: array in which cropped contours to be saved\n",
    "    '''\n",
    "    total_contours = len(contours)\n",
    "    rect_color = (255,0,0)\n",
    "    rect_stroke_width = 2\n",
    "    \n",
    "    new_img = image.copy()\n",
    "    \n",
    "    for i in range(total_contours):\n",
    "        cnt = contours[i]\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        image = cv2.rectangle(image, (x,y), (x+w,y+h), rect_color, rect_stroke_width)\n",
    "        \n",
    "        # Step 5: Cropping individual contours and saving them in array\n",
    "        cropping_rectangles_and_saving_them(x,y,x+w,y+h,new_img,i,arr_contours)\n",
    "        \n",
    "    # To visualize all contours on image\n",
    "    #plt.imshow(image)\n",
    "    #plt.show()\n",
    "      \n",
    "\n",
    "def cropping_rectangles_and_saving_them(left,upper,right,lower, image_name, cropped_image_name, arr_contours):\n",
    "    '''\n",
    "    This function will crop all the bounding rectangles from an image\n",
    "    Parameters:\n",
    "    left=x,upper=y,right=x+w,lower=y+h\n",
    "    image_name: original image\n",
    "    cropped_image_name: individual cropped images name\n",
    "    '''\n",
    "    # Opens image using PIL\n",
    "    im = Image.fromarray(image_name)\n",
    "    \n",
    "    # Crop image from original image\n",
    "    box = (left,upper,right,lower)\n",
    "    cropped_image = im.crop(box)\n",
    "    \n",
    "    # Appending cropped contours\n",
    "    arr_contours.append(cropped_image)\n",
    "    \n",
    "    \n",
    "def resizing_cropped_img_to_20_by_20(arr_cropped_images):\n",
    "    '''\n",
    "    This function will resize all the cropped images to 20x20\n",
    "    Parameters:\n",
    "    arr_cropped_images: arr containing all cropped numbers\n",
    "    '''\n",
    "    new_Arr = []\n",
    "    \n",
    "    for i in range(len(arr_cropped_images)):\n",
    "        # Step:6 Resize img to 20 by 20\n",
    "        new_Arr.append(arr_cropped_images[i].resize((20,20)))\n",
    "        \n",
    "    return new_Arr\n",
    "    \n",
    "    \n",
    "def add_borders_to_img(arr_20x20_imgs):\n",
    "    '''\n",
    "    This function will resize all the cropped images to 28x28\n",
    "    Parameters:\n",
    "    arr_20x20_imgs: arr containing all 20x20 images\n",
    "    '''\n",
    "    new_Arr = []\n",
    "    \n",
    "    for i in range(len(arr_20x20_imgs)):\n",
    "        old_size = arr_20x20_imgs[i].size\n",
    "        new_size = (28,28)\n",
    "        \n",
    "        img_28x28 = Image.new('RGB',new_size, color=(255,255,255))\n",
    "        img_28x28.paste(arr_20x20_imgs[i],((new_size[0]-old_size[0])//2,(new_size[1]-old_size[1])//2))\n",
    "        new_Arr.append(img_28x28)\n",
    "        \n",
    "    return new_Arr \n",
    "\n",
    "def ret_x_cord_contour(contours):\n",
    "    '''\n",
    "    This func will get x-cord for contour\n",
    "    Parameters:\n",
    "    contours: Array containing all contours\n",
    "    '''\n",
    "    if int(cv2.contourArea(contours)) > 0:\n",
    "        cent_moment = cv2.moments(contours)\n",
    "        return cent_moment['m10']/cent_moment['m00']\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def get_contours_of_the_image(image, dilation=False):\n",
    "    '''\n",
    "    This func will find contours on image\n",
    "    And return them in sorted order\n",
    "    Parameters:\n",
    "    image: Image \n",
    "    dilation: Boolean value\n",
    "    '''\n",
    "    # Convert binary to RGB\n",
    "    img = image.convert('RGB')\n",
    "    img = np.array(img)\n",
    "\n",
    "    # convert the image to grayscale format\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # apply thresholding\n",
    "    ret, thresh = cv2.threshold(img_gray, 150, 255, cv2.THRESH_OTSU|cv2.THRESH_BINARY)\n",
    "\n",
    "    if dilation:\n",
    "        # Finding columns(words)\n",
    "        rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (30,1))\n",
    "        dilation = cv2.dilate(thresh, rect_kernel, iterations=1)\n",
    "\n",
    "        # detect the contours on the binary image using cv2.CHAIN_APPROX_NONE\n",
    "        contours, hierarchy = cv2.findContours(image=dilation, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE)\n",
    "    else:\n",
    "        # detect the contours on the binary image using cv2.CHAIN_APPROX_NONE\n",
    "        contours, hierarchy = cv2.findContours(image=thresh, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE)\n",
    "        \n",
    "    # Sort contours from left->Right\n",
    "    contours = sorted(contours, key=ret_x_cord_contour, reverse=False)\n",
    "    \n",
    "    return contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image using opencv\n",
    "img = cv2.imread(SAMPLE_IMAGE)\n",
    "\n",
    "# convert the image to grayscale format\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# apply thresholding\n",
    "ret, thresh = cv2.threshold(img_gray, 150, 255, cv2.THRESH_OTSU|cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# ----------------------------------\n",
    "# 1. FINDING TOTAL ROWS\n",
    "# ----------------------------------\n",
    "rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (120,10))\n",
    "dilation = cv2.dilate(thresh, rect_kernel, iterations=1)\n",
    "\n",
    "# detect the contours on the binary image using cv2.CHAIN_APPROX_NONE\n",
    "contours, hierarchy = cv2.findContours(image=dilation, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE)\n",
    " \n",
    "if len(contours) > 0:\n",
    "    # start from 1st row\n",
    "    contours.reverse()\n",
    "    \n",
    "    # Crop all rows\n",
    "    ARR_CROPPED_ROWS = []\n",
    "    draw_rectangle_on_img_contours_and_croppping_them(contours, thresh.copy(),ARR_CROPPED_ROWS)\n",
    "    \n",
    "    # ------------------------------------\n",
    "    # 2. FINDING COLS(WORDS) IN EVERY ROW\n",
    "    # ------------------------------------\n",
    "    \n",
    "    if len(ARR_CROPPED_ROWS) > 0:\n",
    "        for i in range(len(ARR_CROPPED_ROWS)):\n",
    "            # Convert binary to RGB\n",
    "            img = ARR_CROPPED_ROWS[i].convert('RGB')\n",
    "            img = np.array(img)\n",
    "            \n",
    "            # convert the image to grayscale format\n",
    "            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # apply thresholding\n",
    "            ret, thresh = cv2.threshold(img_gray, 150, 255, cv2.THRESH_OTSU|cv2.THRESH_BINARY)\n",
    "            \n",
    "            # Finding columns(words)\n",
    "            rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (50,1))\n",
    "            dilation = cv2.dilate(thresh, rect_kernel, iterations=1)\n",
    "            \n",
    "            # detect the contours on the binary image using cv2.CHAIN_APPROX_NONE\n",
    "            contours, hierarchy = cv2.findContours(image=dilation, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE)\n",
    "            \n",
    "            # Sort contours from left->Right\n",
    "            contours = sorted(contours, key=ret_x_cord_contour, reverse=False)\n",
    "            \n",
    "            # Crop all words\n",
    "            ARR_CROPPED_WORDS = []\n",
    "            draw_rectangle_on_img_contours_and_croppping_them(contours, thresh.copy(),ARR_CROPPED_WORDS)\n",
    "\n",
    "            # ------------------------------------\n",
    "            # 3. FINDING CHARS IN EVERY WORD\n",
    "            # ------------------------------------\n",
    "            ARR_FINAL_WORDS = []\n",
    "            \n",
    "            if len(ARR_CROPPED_WORDS) > 0:\n",
    "                for j in range(len(ARR_CROPPED_WORDS)):\n",
    "                    # Read image using opencv\n",
    "                    img = ARR_CROPPED_WORDS[j].convert('RGB')\n",
    "                    img = np.array(img)\n",
    "\n",
    "                    # convert the image to grayscale format\n",
    "                    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    # apply thresholding\n",
    "                    ret, thresh = cv2.threshold(img_gray, 150, 255, cv2.THRESH_OTSU|cv2.THRESH_BINARY)\n",
    "\n",
    "                    # detect the contours on the binary image using cv2.CHAIN_APPROX_NONE\n",
    "                    contours, hierarchy = cv2.findContours(image=thresh, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE)\n",
    "           \n",
    "                    # Sort contours from left->Right\n",
    "                    contours = sorted(contours, key=ret_x_cord_contour, reverse=False)\n",
    "                \n",
    "                    # Crop all chars\n",
    "                    ARR_CROPPED_CHARS = []\n",
    "                    draw_rectangle_on_img_contours_and_croppping_them(contours, thresh.copy(),ARR_CROPPED_CHARS)\n",
    "                    \n",
    "                    # \n",
    "                    ARR_CROPPED_CHARS_NEW = []\n",
    "                    for img in ARR_CROPPED_CHARS:\n",
    "                        img = np.array(img)\n",
    "                        img = cv2.bitwise_not(img)\n",
    "                        img = Image.fromarray(img)\n",
    "                        ARR_CROPPED_CHARS_NEW.append(img)\n",
    "                        \n",
    "                    \n",
    "                    # ------------------------------------------------\n",
    "                    # STEP 3.1: CONVERT CROPPED DIGITS TO 20X20\n",
    "                    # ------------------------------------------------\n",
    "                    arr_img_20x20 = resizing_cropped_img_to_20_by_20(ARR_CROPPED_CHARS_NEW)\n",
    "\n",
    "                    # ------------------------------------------------\n",
    "                    # STEP 3.2: CONVERT 20x20 TO 28X28 AND REVERSE IT\n",
    "                    # ------------------------------------------------\n",
    "                    arr_img_28x28 = add_borders_to_img(arr_img_20x20)\n",
    "                    \n",
    "                    # ------------------------------------\n",
    "                    # 4. APPENDING CHARS TO WORDS ARRAY\n",
    "                    # ------------------------------------\n",
    "                    ARR_FINAL_WORDS.append(arr_img_28x28)\n",
    "                    \n",
    "                # ------------------------------------\n",
    "                # 5. APPENDING WORDS ARRAY TO ROWS ARRAY\n",
    "                # ------------------------------------\n",
    "                ARR_FINAL_ROWS.append(ARR_FINAL_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = ARR_FINAL_ROWS[3][0][0]  #word = row,cols ;returns chars array of that word\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-884470e6aa96>:29: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from SavedModel/variables/variables\n"
     ]
    }
   ],
   "source": [
    "ARR_FINAL_PREDICTION = []\n",
    "for row in range(len(ARR_FINAL_ROWS)):\n",
    "    \n",
    "    ARR_FINAL_PREDICTION_ROWS = []\n",
    "    for col in range(len(ARR_FINAL_ROWS[row])):\n",
    "        \n",
    "        ARR_FINAL_PREDICTION_WORDS = []\n",
    "        for char in range(len(ARR_FINAL_ROWS[row][col])):\n",
    "            # ------------------------------------------------\n",
    "            # STEP 6: CONVERT 28x28 TO FLAT ARRAY\n",
    "            # ------------------------------------------------\n",
    "            x_test = []\n",
    "            \n",
    "            bw = ARR_FINAL_ROWS[row][col][char].convert('L')\n",
    "            img_array = np.invert(bw)\n",
    "            test_img = img_array.ravel()\n",
    "            x_test.append(test_img)\n",
    "            \n",
    "            # ------------------------------------------------\n",
    "            # STEP 7: LOAD MODEL AND CREATE SESSION\n",
    "            # ------------------------------------------------\n",
    "            # Create a graph obj placeholder\n",
    "            graph = tf.Graph()\n",
    "\n",
    "            # Creating a sess obj and linking session and the graph\n",
    "            sess = tf.compat.v1.Session(graph=graph)\n",
    "\n",
    "            # Loading the Model\n",
    "            load(sess=sess, tags=[tag_constants.SERVING], export_dir='SavedModel')\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # STEP 8: MAKING PREDICTIONS\n",
    "            # ------------------------------------------------\n",
    "            # Return the `Tensor` with the given `name` and index=0 result\n",
    "            X = graph.get_tensor_by_name('X:0')\n",
    "\n",
    "            # Get hold of the tensor that will hold the predictions from the graph. Store these under y_pred\n",
    "            y_pred = graph.get_tensor_by_name('accuracy_calc/prediction:0')\n",
    "\n",
    "            # fetches = y_pred(the output we are after)\n",
    "            prediction = sess.run(fetches=y_pred, feed_dict={X: x_test})\n",
    "            \n",
    "            # ------------------------------------\n",
    "            # 9. APPENDING CHARS TO WORDS ARRAY\n",
    "            # ------------------------------------\n",
    "            ARR_FINAL_PREDICTION_WORDS.append(prediction)\n",
    "            \n",
    "        ARR_FINAL_PREDICTION_ROWS.append(ARR_FINAL_PREDICTION_WORDS)\n",
    "        \n",
    "    ARR_FINAL_PREDICTION.append(ARR_FINAL_PREDICTION_ROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ARR_FINAL_PREDICTION[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# 10. DECODING ALHPABETS AND CONCATENATIING\n",
    "#     THEM INTO STRING\n",
    "# -----------------------------------------\n",
    "result = ''\n",
    "for row in range(len(ARR_FINAL_PREDICTION)):\n",
    "    for col in range(len(ARR_FINAL_PREDICTION[row])):\n",
    "        for char in range(len(ARR_FINAL_PREDICTION[row][col])):\n",
    "            result = result + ALPHABETS[ARR_FINAL_PREDICTION[row][col][char][0]]\n",
    "            \n",
    "        result = result + ' '\n",
    "        \n",
    "    result = result + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYED FASQUE ALE \n",
      "BSCS \n",
      "SECTSON A \n",
      "DEV \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
